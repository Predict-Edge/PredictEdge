{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gold Price Prediction with LSTM (End-to-End)\n",
        "\n",
        "This notebook builds a complete, production-ready pipeline to predict daily gold prices using Long Short-Term Memory (LSTM) neural networks. It includes:\n",
        "\n",
        "- Data collection from `yfinance` and `FRED` (via `pandas_datareader`)\n",
        "- Feature engineering and preprocessing\n",
        "- Chronological train/validation/test splitting\n",
        "- LSTM modeling with callbacks and model persistence\n",
        "- Evaluation with multiple metrics and publication-ready visualizations\n",
        "- 7-day future prediction with uncertainty bounds\n",
        "\n",
        "We follow best practices for time series modeling and include robust error handling and fallbacks where necessary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Environment initialized.\n"
          ]
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# ## Setup and Imports\n",
        "# - Set random seeds for reproducibility\n",
        "# - Import required libraries\n",
        "# - Configure plotting styles\n",
        "# - Define utility functions used throughout the notebook\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Print progress helper\n",
        "def log(msg: str) -> None:\n",
        "    print(f\"[INFO] {msg}\")\n",
        "\n",
        "# Reproducibility\n",
        "import random\n",
        "import numpy as np\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    tf.keras.utils.set_random_seed(42)\n",
        "    tf.config.experimental.enable_op_determinism()\n",
        "except Exception as e:\n",
        "    log(\"TensorFlow determinism not fully enabled; continuing.\")\n",
        "\n",
        "# Core libs\n",
        "import pandas as pd\n",
        "from pandas import DataFrame, Series\n",
        "\n",
        "# Data sources\n",
        "try:\n",
        "    import yfinance as yf\n",
        "except Exception as e:\n",
        "    raise ImportError(\"Please install yfinance: pip install yfinance\")\n",
        "\n",
        "try:\n",
        "    from pandas_datareader import data as pdr\n",
        "except Exception:\n",
        "    raise ImportError(\"Please install pandas_datareader: pip install pandas_datareader\")\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "plt.rcParams[\"axes.grid\"] = True\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# Paths\n",
        "PROJECT_ROOT = \"/Users/keerat/Desktop/Projects/Predict Edge/PredictEdge\"\n",
        "RAW_CSV_PATH = os.path.join(PROJECT_ROOT, \"gold_raw_data.csv\")\n",
        "PREP_CSV_PATH = os.path.join(PROJECT_ROOT, \"gold_preprocessed_data.csv\")\n",
        "MODEL_PATH = os.path.join(PROJECT_ROOT, \"gold_price_lstm_model.h5\")\n",
        "\n",
        "log(\"Environment initialized.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Utility functions loaded.\n"
          ]
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# ### Utility Functions\n",
        "# Helper utilities for downloads, indicators, lag creation, sequences, and metrics.\n",
        "\n",
        "from typing import Tuple, List, Optional, Dict\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# ----------------------------------------\n",
        "# Download helpers with retries and fallbacks\n",
        "# ----------------------------------------\n",
        "def download_yfinance(ticker: str, start: str, end: str, interval: str = \"1d\", max_retries: int = 3) -> DataFrame:\n",
        "    \"\"\"Download data from yfinance with basic retries.\n",
        "    Returns a DataFrame with a DatetimeIndex. If download fails, returns synthetic series.\n",
        "    \"\"\"\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            log(f\"Downloading {ticker} from yfinance (attempt {attempt})...\")\n",
        "            df = yf.download(ticker, start=start, end=end, interval=interval, auto_adjust=True, progress=False)\n",
        "            if not df.empty:\n",
        "                df = df.rename(columns={\"Close\": ticker}).loc[:, [ticker]]\n",
        "                return df\n",
        "        except Exception as e:\n",
        "            log(f\"Download error for {ticker}: {e}\")\n",
        "    # Synthetic fallback (random walk)\n",
        "    log(f\"Falling back to synthetic data for {ticker}.\")\n",
        "    dates = pd.date_range(start=start, end=end, freq=\"B\")\n",
        "    rnd = np.random.default_rng(42)\n",
        "    steps = rnd.normal(0, 1, size=len(dates))\n",
        "    series = 100 + np.cumsum(steps)\n",
        "    return pd.DataFrame(series, index=dates, columns=[ticker])\n",
        "\n",
        "\n",
        "def download_fred(series_id: str, start: str, end: str, max_retries: int = 3) -> DataFrame:\n",
        "    \"\"\"Download data from FRED via pandas_datareader. Falls back to synthetic if needed.\"\"\"\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            log(f\"Downloading {series_id} from FRED (attempt {attempt})...\")\n",
        "            df = pdr.DataReader(series_id, \"fred\", start, end)\n",
        "            if not df.empty:\n",
        "                df = df.rename(columns={series_id: series_id})\n",
        "                return df\n",
        "        except Exception as e:\n",
        "            log(f\"FRED download error for {series_id}: {e}\")\n",
        "    # Synthetic monthly CPI-like trend\n",
        "    log(f\"Falling back to synthetic data for {series_id}.\")\n",
        "    dates = pd.date_range(start=start, end=end, freq=\"MS\")\n",
        "    trend = np.linspace(250, 300, num=len(dates))\n",
        "    noise = np.random.default_rng(42).normal(0, 0.3, size=len(dates))\n",
        "    series = trend + noise\n",
        "    return pd.DataFrame(series, index=dates, columns=[series_id])\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# Indicators and feature engineering\n",
        "# ----------------------------------------\n",
        "def compute_rsi(series: Series, period: int = 14) -> Series:\n",
        "    \"\"\"Compute Relative Strength Index (RSI).\"\"\"\n",
        "    delta = series.diff()\n",
        "    gain = delta.clip(lower=0.0)\n",
        "    loss = -delta.clip(upper=0.0)\n",
        "    avg_gain = gain.rolling(window=period, min_periods=period).mean()\n",
        "    avg_loss = loss.rolling(window=period, min_periods=period).mean()\n",
        "    rs = avg_gain / (avg_loss.replace(0, np.nan))\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi.fillna(method=\"bfill\").fillna(50.0)\n",
        "\n",
        "\n",
        "def add_lag_features(df: DataFrame, columns: List[str], lags: List[int]) -> DataFrame:\n",
        "    \"\"\"Create lag features for given columns and lags.\"\"\"\n",
        "    out = df.copy()\n",
        "    for col in columns:\n",
        "        for l in lags:\n",
        "            out[f\"{col}_lag_{l}\"] = out[col].shift(l)\n",
        "    return out\n",
        "\n",
        "\n",
        "def create_sequences(features: np.ndarray, targets: np.ndarray, seq_len: int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Create LSTM sequences of length seq_len from aligned feature and target arrays.\"\"\"\n",
        "    X_list, y_list = [], []\n",
        "    for i in range(seq_len, len(features)):\n",
        "        X_list.append(features[i - seq_len:i])\n",
        "        y_list.append(targets[i])\n",
        "    return np.array(X_list), np.array(y_list)\n",
        "\n",
        "\n",
        "def mape(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    y_true = np.array(y_true).astype(float)\n",
        "    y_pred = np.array(y_pred).astype(float)\n",
        "    nonzero = np.where(y_true != 0)[0]\n",
        "    if len(nonzero) == 0:\n",
        "        return np.nan\n",
        "    return np.mean(np.abs((y_true[nonzero] - y_pred[nonzero]) / y_true[nonzero])) * 100\n",
        "\n",
        "\n",
        "def print_metrics_table(results: Dict[str, Dict[str, float]]) -> None:\n",
        "    \"\"\"Print a formatted metrics table for train/val/test.\"\"\"\n",
        "    df = pd.DataFrame(results).T[[\"RMSE\", \"MAE\", \"MAPE\", \"R2\"]]\n",
        "    print(\"\\nEvaluation Metrics:\")\n",
        "    print(df.to_string(float_format=lambda x: f\"{x:,.4f}\"))\n",
        "\n",
        "\n",
        "log(\"Utility functions loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Data Collection\n",
        "\n",
        "We will download 10 years of historical data for:\n",
        "- Gold Futures (`GC=F`) as the target series\n",
        "- Economic indicators: `^TNX`, `DX-Y.NYB`, `^GSPC`, `^VIX`, `CL=F`, `SI=F`\n",
        "- CPI from FRED (`CPIAUCSL`)\n",
        "\n",
        "All datasets will be merged on date and forward-filled to handle missing values. If any API fails, we will fallback to synthetic data to keep the pipeline runnable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Downloading GC=F from yfinance (attempt 1)...\n",
            "[INFO] Downloading ^TNX from yfinance (attempt 1)...\n",
            "[INFO] Downloading DX-Y.NYB from yfinance (attempt 1)...\n",
            "[INFO] Downloading ^GSPC from yfinance (attempt 1)...\n",
            "[INFO] Downloading ^VIX from yfinance (attempt 1)...\n",
            "[INFO] Downloading CL=F from yfinance (attempt 1)...\n",
            "[INFO] Downloading SI=F from yfinance (attempt 1)...\n",
            "[INFO] Downloading CPIAUCSL from FRED (attempt 1)...\n",
            "[INFO] Raw data saved to /Users/keerat/Desktop/Projects/Predict Edge/PredictEdge/gold_raw_data.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gold_Gold</th>\n",
              "      <th>TNX_10Y_TNX_10Y</th>\n",
              "      <th>DXY_DXY</th>\n",
              "      <th>SP500_SP500</th>\n",
              "      <th>VIX_VIX</th>\n",
              "      <th>CrudeOil_CrudeOil</th>\n",
              "      <th>Silver_Silver</th>\n",
              "      <th>CPIAUCSL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2015-10-30</th>\n",
              "      <td>1141.500000</td>\n",
              "      <td>2.151</td>\n",
              "      <td>96.949997</td>\n",
              "      <td>2079.360107</td>\n",
              "      <td>15.07</td>\n",
              "      <td>46.590000</td>\n",
              "      <td>15.566</td>\n",
              "      <td>238.017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-11-02</th>\n",
              "      <td>1135.800049</td>\n",
              "      <td>2.187</td>\n",
              "      <td>96.930000</td>\n",
              "      <td>2104.050049</td>\n",
              "      <td>14.15</td>\n",
              "      <td>46.139999</td>\n",
              "      <td>15.413</td>\n",
              "      <td>238.017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-11-03</th>\n",
              "      <td>1114.199951</td>\n",
              "      <td>2.220</td>\n",
              "      <td>97.160004</td>\n",
              "      <td>2109.790039</td>\n",
              "      <td>14.54</td>\n",
              "      <td>47.900002</td>\n",
              "      <td>15.244</td>\n",
              "      <td>238.017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-11-04</th>\n",
              "      <td>1106.500000</td>\n",
              "      <td>2.230</td>\n",
              "      <td>97.949997</td>\n",
              "      <td>2102.310059</td>\n",
              "      <td>15.51</td>\n",
              "      <td>46.320000</td>\n",
              "      <td>15.063</td>\n",
              "      <td>238.017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-11-05</th>\n",
              "      <td>1104.400024</td>\n",
              "      <td>2.245</td>\n",
              "      <td>97.940002</td>\n",
              "      <td>2099.929932</td>\n",
              "      <td>15.05</td>\n",
              "      <td>45.200001</td>\n",
              "      <td>14.988</td>\n",
              "      <td>238.017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Gold_Gold  TNX_10Y_TNX_10Y    DXY_DXY  SP500_SP500  VIX_VIX  \\\n",
              "2015-10-30  1141.500000            2.151  96.949997  2079.360107    15.07   \n",
              "2015-11-02  1135.800049            2.187  96.930000  2104.050049    14.15   \n",
              "2015-11-03  1114.199951            2.220  97.160004  2109.790039    14.54   \n",
              "2015-11-04  1106.500000            2.230  97.949997  2102.310059    15.51   \n",
              "2015-11-05  1104.400024            2.245  97.940002  2099.929932    15.05   \n",
              "\n",
              "            CrudeOil_CrudeOil  Silver_Silver  CPIAUCSL  \n",
              "2015-10-30          46.590000         15.566   238.017  \n",
              "2015-11-02          46.139999         15.413   238.017  \n",
              "2015-11-03          47.900002         15.244   238.017  \n",
              "2015-11-04          46.320000         15.063   238.017  \n",
              "2015-11-05          45.200001         14.988   238.017  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gold_Gold</th>\n",
              "      <th>TNX_10Y_TNX_10Y</th>\n",
              "      <th>DXY_DXY</th>\n",
              "      <th>SP500_SP500</th>\n",
              "      <th>VIX_VIX</th>\n",
              "      <th>CrudeOil_CrudeOil</th>\n",
              "      <th>Silver_Silver</th>\n",
              "      <th>CPIAUCSL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2025-10-23</th>\n",
              "      <td>4125.500000</td>\n",
              "      <td>3.991</td>\n",
              "      <td>98.940002</td>\n",
              "      <td>6738.439941</td>\n",
              "      <td>17.299999</td>\n",
              "      <td>61.790001</td>\n",
              "      <td>48.481998</td>\n",
              "      <td>324.368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-24</th>\n",
              "      <td>4118.399902</td>\n",
              "      <td>3.997</td>\n",
              "      <td>98.949997</td>\n",
              "      <td>6791.689941</td>\n",
              "      <td>16.370001</td>\n",
              "      <td>61.500000</td>\n",
              "      <td>48.376999</td>\n",
              "      <td>324.368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-27</th>\n",
              "      <td>4001.899902</td>\n",
              "      <td>3.997</td>\n",
              "      <td>98.779999</td>\n",
              "      <td>6875.160156</td>\n",
              "      <td>15.790000</td>\n",
              "      <td>61.310001</td>\n",
              "      <td>46.562000</td>\n",
              "      <td>324.368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-28</th>\n",
              "      <td>3966.199951</td>\n",
              "      <td>3.983</td>\n",
              "      <td>98.690002</td>\n",
              "      <td>6890.890137</td>\n",
              "      <td>16.420000</td>\n",
              "      <td>60.150002</td>\n",
              "      <td>47.125000</td>\n",
              "      <td>324.368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-29</th>\n",
              "      <td>3983.699951</td>\n",
              "      <td>4.058</td>\n",
              "      <td>99.220001</td>\n",
              "      <td>6890.589844</td>\n",
              "      <td>16.920000</td>\n",
              "      <td>60.480000</td>\n",
              "      <td>47.721001</td>\n",
              "      <td>324.368</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Gold_Gold  TNX_10Y_TNX_10Y    DXY_DXY  SP500_SP500    VIX_VIX  \\\n",
              "2025-10-23  4125.500000            3.991  98.940002  6738.439941  17.299999   \n",
              "2025-10-24  4118.399902            3.997  98.949997  6791.689941  16.370001   \n",
              "2025-10-27  4001.899902            3.997  98.779999  6875.160156  15.790000   \n",
              "2025-10-28  3966.199951            3.983  98.690002  6890.890137  16.420000   \n",
              "2025-10-29  3983.699951            4.058  99.220001  6890.589844  16.920000   \n",
              "\n",
              "            CrudeOil_CrudeOil  Silver_Silver  CPIAUCSL  \n",
              "2025-10-23          61.790001      48.481998   324.368  \n",
              "2025-10-24          61.500000      48.376999   324.368  \n",
              "2025-10-27          61.310001      46.562000   324.368  \n",
              "2025-10-28          60.150002      47.125000   324.368  \n",
              "2025-10-29          60.480000      47.721001   324.368  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape: (2609, 8)\n"
          ]
        }
      ],
      "source": [
        "# Data Collection with robust error handling\n",
        "START_DATE = (pd.Timestamp.today() - pd.DateOffset(years=10)).strftime(\"%Y-%m-%d\")\n",
        "END_DATE = pd.Timestamp.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "TICKERS = {\n",
        "    \"GC=F\": \"Gold\",\n",
        "    \"^TNX\": \"TNX_10Y\",\n",
        "    \"DX-Y.NYB\": \"DXY\",\n",
        "    \"^GSPC\": \"SP500\",\n",
        "    \"^VIX\": \"VIX\",\n",
        "    \"CL=F\": \"CrudeOil\",\n",
        "    \"SI=F\": \"Silver\",\n",
        "}\n",
        "\n",
        "fred_series = \"CPIAUCSL\"\n",
        "\n",
        "try:\n",
        "    # Download market series from yfinance\n",
        "    frames = []\n",
        "    for tkr, alias in TICKERS.items():\n",
        "        df_t = download_yfinance(tkr, START_DATE, END_DATE)\n",
        "        df_t = df_t.rename(columns={tkr: alias})\n",
        "        frames.append(df_t)\n",
        "\n",
        "    # Merge market data on date index\n",
        "    market_df = pd.concat(frames, axis=1)\n",
        "\n",
        "    # CPI is monthly; we'll forward-fill to daily\n",
        "    cpi_df = download_fred(fred_series, START_DATE, END_DATE)\n",
        "    cpi_daily = cpi_df.resample(\"B\").ffill()\n",
        "\n",
        "    # Flatten MultiIndex columns (if any)\n",
        "    if isinstance(market_df.columns, pd.MultiIndex):\n",
        "        market_df.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in market_df.columns]\n",
        "\n",
        "\n",
        "    # Merge all\n",
        "    data = market_df.join(cpi_daily, how=\"outer\")\n",
        "    data = data.sort_index()\n",
        "\n",
        "    # Handle missing values via forward fill then backfill\n",
        "    data = data.ffill().bfill()\n",
        "\n",
        "    # Basic sanity checks\n",
        "    assert not data.isna().sum().sum(), \"Missing values remain after fill.\"\n",
        "    assert data.shape[0] > 0 and data.shape[1] >= 7, \"Insufficient data collected.\"\n",
        "\n",
        "    # Save raw data\n",
        "    data.to_csv(RAW_CSV_PATH, index=True)\n",
        "    log(f\"Raw data saved to {RAW_CSV_PATH}\")\n",
        "\n",
        "    display(data.head())\n",
        "    display(data.tail())\n",
        "    print(\"Data shape:\", data.shape)\n",
        "except Exception as e:\n",
        "    log(f\"Data collection failed: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Preprocessing and Feature Engineering\n",
        "\n",
        "We'll create technical indicators for gold, compute returns and volatility, add RSI, and generate lagged features for all numeric columns. Finally, we'll scale features to [0, 1] using `MinMaxScaler` and save the preprocessed dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before rename: ['Gold_Gold', 'TNX_10Y_TNX_10Y', 'DXY_DXY', 'SP500_SP500', 'VIX_VIX', 'CrudeOil_CrudeOil', 'Silver_Silver', 'CPIAUCSL']\n",
            "After rename: ['Gold', 'TNX', 'DXY', 'SP500', 'VIX', 'CrudeOil', 'Silver', 'CPIAUCSL']\n",
            "[INFO] Preprocessed data saved to /Users/keerat/Desktop/Projects/Predict Edge/PredictEdge/gold_preprocessed_data.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gold</th>\n",
              "      <th>TNX</th>\n",
              "      <th>DXY</th>\n",
              "      <th>SP500</th>\n",
              "      <th>VIX</th>\n",
              "      <th>CrudeOil</th>\n",
              "      <th>Silver</th>\n",
              "      <th>CPIAUCSL</th>\n",
              "      <th>Gold_MA_7</th>\n",
              "      <th>Gold_MA_30</th>\n",
              "      <th>Gold_MA_90</th>\n",
              "      <th>Gold_Returns</th>\n",
              "      <th>Gold_Volatility_30</th>\n",
              "      <th>Gold_RSI_14</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2015-10-30</th>\n",
              "      <td>1141.500000</td>\n",
              "      <td>2.151</td>\n",
              "      <td>96.949997</td>\n",
              "      <td>2079.360107</td>\n",
              "      <td>15.07</td>\n",
              "      <td>46.590000</td>\n",
              "      <td>15.566</td>\n",
              "      <td>238.017</td>\n",
              "      <td>1111.128575</td>\n",
              "      <td>1081.990002</td>\n",
              "      <td>1116.790001</td>\n",
              "      <td>-0.004993</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>14.365822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-11-02</th>\n",
              "      <td>1135.800049</td>\n",
              "      <td>2.187</td>\n",
              "      <td>96.930000</td>\n",
              "      <td>2104.050049</td>\n",
              "      <td>14.15</td>\n",
              "      <td>46.139999</td>\n",
              "      <td>15.413</td>\n",
              "      <td>238.017</td>\n",
              "      <td>1111.128575</td>\n",
              "      <td>1081.990002</td>\n",
              "      <td>1116.790001</td>\n",
              "      <td>-0.004993</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>14.365822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-11-03</th>\n",
              "      <td>1114.199951</td>\n",
              "      <td>2.220</td>\n",
              "      <td>97.160004</td>\n",
              "      <td>2109.790039</td>\n",
              "      <td>14.54</td>\n",
              "      <td>47.900002</td>\n",
              "      <td>15.244</td>\n",
              "      <td>238.017</td>\n",
              "      <td>1111.128575</td>\n",
              "      <td>1081.990002</td>\n",
              "      <td>1116.790001</td>\n",
              "      <td>-0.019018</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>14.365822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-11-04</th>\n",
              "      <td>1106.500000</td>\n",
              "      <td>2.230</td>\n",
              "      <td>97.949997</td>\n",
              "      <td>2102.310059</td>\n",
              "      <td>15.51</td>\n",
              "      <td>46.320000</td>\n",
              "      <td>15.063</td>\n",
              "      <td>238.017</td>\n",
              "      <td>1111.128575</td>\n",
              "      <td>1081.990002</td>\n",
              "      <td>1116.790001</td>\n",
              "      <td>-0.006911</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>14.365822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-11-05</th>\n",
              "      <td>1104.400024</td>\n",
              "      <td>2.245</td>\n",
              "      <td>97.940002</td>\n",
              "      <td>2099.929932</td>\n",
              "      <td>15.05</td>\n",
              "      <td>45.200001</td>\n",
              "      <td>14.988</td>\n",
              "      <td>238.017</td>\n",
              "      <td>1111.128575</td>\n",
              "      <td>1081.990002</td>\n",
              "      <td>1116.790001</td>\n",
              "      <td>-0.001898</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>14.365822</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Gold    TNX        DXY        SP500    VIX   CrudeOil  \\\n",
              "2015-10-30  1141.500000  2.151  96.949997  2079.360107  15.07  46.590000   \n",
              "2015-11-02  1135.800049  2.187  96.930000  2104.050049  14.15  46.139999   \n",
              "2015-11-03  1114.199951  2.220  97.160004  2109.790039  14.54  47.900002   \n",
              "2015-11-04  1106.500000  2.230  97.949997  2102.310059  15.51  46.320000   \n",
              "2015-11-05  1104.400024  2.245  97.940002  2099.929932  15.05  45.200001   \n",
              "\n",
              "            Silver  CPIAUCSL    Gold_MA_7   Gold_MA_30   Gold_MA_90  \\\n",
              "2015-10-30  15.566   238.017  1111.128575  1081.990002  1116.790001   \n",
              "2015-11-02  15.413   238.017  1111.128575  1081.990002  1116.790001   \n",
              "2015-11-03  15.244   238.017  1111.128575  1081.990002  1116.790001   \n",
              "2015-11-04  15.063   238.017  1111.128575  1081.990002  1116.790001   \n",
              "2015-11-05  14.988   238.017  1111.128575  1081.990002  1116.790001   \n",
              "\n",
              "            Gold_Returns  Gold_Volatility_30  Gold_RSI_14  \n",
              "2015-10-30     -0.004993             0.00819    14.365822  \n",
              "2015-11-02     -0.004993             0.00819    14.365822  \n",
              "2015-11-03     -0.019018             0.00819    14.365822  \n",
              "2015-11-04     -0.006911             0.00819    14.365822  \n",
              "2015-11-05     -0.001898             0.00819    14.365822  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gold</th>\n",
              "      <th>TNX</th>\n",
              "      <th>DXY</th>\n",
              "      <th>SP500</th>\n",
              "      <th>VIX</th>\n",
              "      <th>CrudeOil</th>\n",
              "      <th>Silver</th>\n",
              "      <th>CPIAUCSL</th>\n",
              "      <th>Gold_MA_7</th>\n",
              "      <th>Gold_MA_30</th>\n",
              "      <th>...</th>\n",
              "      <th>Gold_Volatility_30_lag_1</th>\n",
              "      <th>Gold_Volatility_30_lag_3</th>\n",
              "      <th>Gold_Volatility_30_lag_5</th>\n",
              "      <th>Gold_Volatility_30_lag_7</th>\n",
              "      <th>Gold_Volatility_30_lag_10</th>\n",
              "      <th>Gold_RSI_14_lag_1</th>\n",
              "      <th>Gold_RSI_14_lag_3</th>\n",
              "      <th>Gold_RSI_14_lag_5</th>\n",
              "      <th>Gold_RSI_14_lag_7</th>\n",
              "      <th>Gold_RSI_14_lag_10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2015-11-13</th>\n",
              "      <td>1080.800049</td>\n",
              "      <td>2.280</td>\n",
              "      <td>98.989998</td>\n",
              "      <td>2023.040039</td>\n",
              "      <td>20.08</td>\n",
              "      <td>40.740002</td>\n",
              "      <td>14.203</td>\n",
              "      <td>238.017</td>\n",
              "      <td>1087.771432</td>\n",
              "      <td>1081.990002</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>14.365822</td>\n",
              "      <td>14.365822</td>\n",
              "      <td>14.365822</td>\n",
              "      <td>14.365822</td>\n",
              "      <td>14.365822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-11-16</th>\n",
              "      <td>1083.699951</td>\n",
              "      <td>2.273</td>\n",
              "      <td>99.440002</td>\n",
              "      <td>2053.189941</td>\n",
              "      <td>18.16</td>\n",
              "      <td>41.740002</td>\n",
              "      <td>14.221</td>\n",
              "      <td>238.017</td>\n",
              "      <td>1084.814279</td>\n",
              "      <td>1081.990002</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>14.365822</td>\n",
              "      <td>14.365822</td>\n",
              "      <td>14.365822</td>\n",
              "      <td>14.365822</td>\n",
              "      <td>14.365822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-11-17</th>\n",
              "      <td>1068.699951</td>\n",
              "      <td>2.261</td>\n",
              "      <td>99.629997</td>\n",
              "      <td>2050.439941</td>\n",
              "      <td>18.84</td>\n",
              "      <td>40.669998</td>\n",
              "      <td>14.170</td>\n",
              "      <td>238.017</td>\n",
              "      <td>1082.114275</td>\n",
              "      <td>1081.990002</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>14.365822</td>\n",
              "      <td>14.365822</td>\n",
              "      <td>14.365822</td>\n",
              "      <td>14.365822</td>\n",
              "      <td>14.365822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-11-18</th>\n",
              "      <td>1068.800049</td>\n",
              "      <td>2.269</td>\n",
              "      <td>99.650002</td>\n",
              "      <td>2083.580078</td>\n",
              "      <td>16.85</td>\n",
              "      <td>40.750000</td>\n",
              "      <td>14.080</td>\n",
              "      <td>238.017</td>\n",
              "      <td>1079.385707</td>\n",
              "      <td>1081.990002</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>14.365822</td>\n",
              "      <td>14.365822</td>\n",
              "      <td>14.365822</td>\n",
              "      <td>14.365822</td>\n",
              "      <td>14.365822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-11-19</th>\n",
              "      <td>1078.000000</td>\n",
              "      <td>2.248</td>\n",
              "      <td>98.989998</td>\n",
              "      <td>2081.239990</td>\n",
              "      <td>16.99</td>\n",
              "      <td>40.540001</td>\n",
              "      <td>14.242</td>\n",
              "      <td>238.017</td>\n",
              "      <td>1077.928571</td>\n",
              "      <td>1081.990002</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>0.00819</td>\n",
              "      <td>14.365822</td>\n",
              "      <td>14.365822</td>\n",
              "      <td>14.365822</td>\n",
              "      <td>14.365822</td>\n",
              "      <td>14.365822</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 84 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Gold    TNX        DXY        SP500    VIX   CrudeOil  \\\n",
              "2015-11-13  1080.800049  2.280  98.989998  2023.040039  20.08  40.740002   \n",
              "2015-11-16  1083.699951  2.273  99.440002  2053.189941  18.16  41.740002   \n",
              "2015-11-17  1068.699951  2.261  99.629997  2050.439941  18.84  40.669998   \n",
              "2015-11-18  1068.800049  2.269  99.650002  2083.580078  16.85  40.750000   \n",
              "2015-11-19  1078.000000  2.248  98.989998  2081.239990  16.99  40.540001   \n",
              "\n",
              "            Silver  CPIAUCSL    Gold_MA_7   Gold_MA_30  ...  \\\n",
              "2015-11-13  14.203   238.017  1087.771432  1081.990002  ...   \n",
              "2015-11-16  14.221   238.017  1084.814279  1081.990002  ...   \n",
              "2015-11-17  14.170   238.017  1082.114275  1081.990002  ...   \n",
              "2015-11-18  14.080   238.017  1079.385707  1081.990002  ...   \n",
              "2015-11-19  14.242   238.017  1077.928571  1081.990002  ...   \n",
              "\n",
              "            Gold_Volatility_30_lag_1  Gold_Volatility_30_lag_3  \\\n",
              "2015-11-13                   0.00819                   0.00819   \n",
              "2015-11-16                   0.00819                   0.00819   \n",
              "2015-11-17                   0.00819                   0.00819   \n",
              "2015-11-18                   0.00819                   0.00819   \n",
              "2015-11-19                   0.00819                   0.00819   \n",
              "\n",
              "            Gold_Volatility_30_lag_5  Gold_Volatility_30_lag_7  \\\n",
              "2015-11-13                   0.00819                   0.00819   \n",
              "2015-11-16                   0.00819                   0.00819   \n",
              "2015-11-17                   0.00819                   0.00819   \n",
              "2015-11-18                   0.00819                   0.00819   \n",
              "2015-11-19                   0.00819                   0.00819   \n",
              "\n",
              "            Gold_Volatility_30_lag_10  Gold_RSI_14_lag_1  Gold_RSI_14_lag_3  \\\n",
              "2015-11-13                    0.00819          14.365822          14.365822   \n",
              "2015-11-16                    0.00819          14.365822          14.365822   \n",
              "2015-11-17                    0.00819          14.365822          14.365822   \n",
              "2015-11-18                    0.00819          14.365822          14.365822   \n",
              "2015-11-19                    0.00819          14.365822          14.365822   \n",
              "\n",
              "            Gold_RSI_14_lag_5  Gold_RSI_14_lag_7  Gold_RSI_14_lag_10  \n",
              "2015-11-13          14.365822          14.365822           14.365822  \n",
              "2015-11-16          14.365822          14.365822           14.365822  \n",
              "2015-11-17          14.365822          14.365822           14.365822  \n",
              "2015-11-18          14.365822          14.365822           14.365822  \n",
              "2015-11-19          14.365822          14.365822           14.365822  \n",
              "\n",
              "[5 rows x 84 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gold</th>\n",
              "      <th>TNX</th>\n",
              "      <th>DXY</th>\n",
              "      <th>SP500</th>\n",
              "      <th>VIX</th>\n",
              "      <th>CrudeOil</th>\n",
              "      <th>Silver</th>\n",
              "      <th>CPIAUCSL</th>\n",
              "      <th>Gold_MA_7</th>\n",
              "      <th>Gold_MA_30</th>\n",
              "      <th>...</th>\n",
              "      <th>Gold_Volatility_30_lag_1</th>\n",
              "      <th>Gold_Volatility_30_lag_3</th>\n",
              "      <th>Gold_Volatility_30_lag_5</th>\n",
              "      <th>Gold_Volatility_30_lag_7</th>\n",
              "      <th>Gold_Volatility_30_lag_10</th>\n",
              "      <th>Gold_RSI_14_lag_1</th>\n",
              "      <th>Gold_RSI_14_lag_3</th>\n",
              "      <th>Gold_RSI_14_lag_5</th>\n",
              "      <th>Gold_RSI_14_lag_7</th>\n",
              "      <th>Gold_RSI_14_lag_10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2015-11-13</th>\n",
              "      <td>0.009131</td>\n",
              "      <td>0.396748</td>\n",
              "      <td>0.407524</td>\n",
              "      <td>0.038318</td>\n",
              "      <td>0.148742</td>\n",
              "      <td>0.485775</td>\n",
              "      <td>0.059775</td>\n",
              "      <td>0.007825</td>\n",
              "      <td>0.007849</td>\n",
              "      <td>0.004375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.140496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-11-16</th>\n",
              "      <td>0.010013</td>\n",
              "      <td>0.395188</td>\n",
              "      <td>0.425157</td>\n",
              "      <td>0.044275</td>\n",
              "      <td>0.122638</td>\n",
              "      <td>0.491973</td>\n",
              "      <td>0.060211</td>\n",
              "      <td>0.007825</td>\n",
              "      <td>0.006903</td>\n",
              "      <td>0.004375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.140496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-11-17</th>\n",
              "      <td>0.005448</td>\n",
              "      <td>0.392515</td>\n",
              "      <td>0.432602</td>\n",
              "      <td>0.043731</td>\n",
              "      <td>0.131883</td>\n",
              "      <td>0.485341</td>\n",
              "      <td>0.058976</td>\n",
              "      <td>0.007825</td>\n",
              "      <td>0.006039</td>\n",
              "      <td>0.004375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.140496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-11-18</th>\n",
              "      <td>0.005478</td>\n",
              "      <td>0.394297</td>\n",
              "      <td>0.433386</td>\n",
              "      <td>0.050278</td>\n",
              "      <td>0.104827</td>\n",
              "      <td>0.485836</td>\n",
              "      <td>0.056796</td>\n",
              "      <td>0.007825</td>\n",
              "      <td>0.005166</td>\n",
              "      <td>0.004375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.140496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-11-19</th>\n",
              "      <td>0.008279</td>\n",
              "      <td>0.389619</td>\n",
              "      <td>0.407524</td>\n",
              "      <td>0.049816</td>\n",
              "      <td>0.106730</td>\n",
              "      <td>0.484535</td>\n",
              "      <td>0.060720</td>\n",
              "      <td>0.007825</td>\n",
              "      <td>0.004699</td>\n",
              "      <td>0.004375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.185908</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.140496</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 84 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                Gold       TNX       DXY     SP500       VIX  CrudeOil  \\\n",
              "2015-11-13  0.009131  0.396748  0.407524  0.038318  0.148742  0.485775   \n",
              "2015-11-16  0.010013  0.395188  0.425157  0.044275  0.122638  0.491973   \n",
              "2015-11-17  0.005448  0.392515  0.432602  0.043731  0.131883  0.485341   \n",
              "2015-11-18  0.005478  0.394297  0.433386  0.050278  0.104827  0.485836   \n",
              "2015-11-19  0.008279  0.389619  0.407524  0.049816  0.106730  0.484535   \n",
              "\n",
              "              Silver  CPIAUCSL  Gold_MA_7  Gold_MA_30  ...  \\\n",
              "2015-11-13  0.059775  0.007825   0.007849    0.004375  ...   \n",
              "2015-11-16  0.060211  0.007825   0.006903    0.004375  ...   \n",
              "2015-11-17  0.058976  0.007825   0.006039    0.004375  ...   \n",
              "2015-11-18  0.056796  0.007825   0.005166    0.004375  ...   \n",
              "2015-11-19  0.060720  0.007825   0.004699    0.004375  ...   \n",
              "\n",
              "            Gold_Volatility_30_lag_1  Gold_Volatility_30_lag_3  \\\n",
              "2015-11-13                  0.185908                  0.185908   \n",
              "2015-11-16                  0.185908                  0.185908   \n",
              "2015-11-17                  0.185908                  0.185908   \n",
              "2015-11-18                  0.185908                  0.185908   \n",
              "2015-11-19                  0.185908                  0.185908   \n",
              "\n",
              "            Gold_Volatility_30_lag_5  Gold_Volatility_30_lag_7  \\\n",
              "2015-11-13                  0.185908                  0.185908   \n",
              "2015-11-16                  0.185908                  0.185908   \n",
              "2015-11-17                  0.185908                  0.185908   \n",
              "2015-11-18                  0.185908                  0.185908   \n",
              "2015-11-19                  0.185908                  0.185908   \n",
              "\n",
              "            Gold_Volatility_30_lag_10  Gold_RSI_14_lag_1  Gold_RSI_14_lag_3  \\\n",
              "2015-11-13                   0.185908           0.140496           0.140496   \n",
              "2015-11-16                   0.185908           0.140496           0.140496   \n",
              "2015-11-17                   0.185908           0.140496           0.140496   \n",
              "2015-11-18                   0.185908           0.140496           0.140496   \n",
              "2015-11-19                   0.185908           0.140496           0.140496   \n",
              "\n",
              "            Gold_RSI_14_lag_5  Gold_RSI_14_lag_7  Gold_RSI_14_lag_10  \n",
              "2015-11-13           0.140496           0.140496            0.140496  \n",
              "2015-11-16           0.140496           0.140496            0.140496  \n",
              "2015-11-17           0.140496           0.140496            0.140496  \n",
              "2015-11-18           0.140496           0.140496            0.140496  \n",
              "2015-11-19           0.140496           0.140496            0.140496  \n",
              "\n",
              "[5 rows x 84 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original with indicators shape: (2609, 14)\n",
            "Lagged shape: (2599, 84)\n",
            "Scaled shape: (2599, 84)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    df = data.copy()\n",
        "\n",
        "    # --- Fix duplicated column names like 'Gold_Gold' -> 'Gold' ---\n",
        "    print(\"Before rename:\", df.columns.tolist())\n",
        "    df.columns = [c.split(\"_\")[0] for c in df.columns]\n",
        "    print(\"After rename:\", df.columns.tolist())\n",
        "\n",
        "    # --- Sanity check ---\n",
        "    assert \"Gold\" in df.columns, f\"'Gold' column not found! Columns: {df.columns.tolist()}\"\n",
        "\n",
        "    # --- Technical indicators specific to gold ---\n",
        "    df[\"Gold_MA_7\"] = df[\"Gold\"].rolling(window=7, min_periods=7).mean()\n",
        "    df[\"Gold_MA_30\"] = df[\"Gold\"].rolling(window=30, min_periods=30).mean()\n",
        "    df[\"Gold_MA_90\"] = df[\"Gold\"].rolling(window=90, min_periods=90).mean()\n",
        "\n",
        "    df[\"Gold_Returns\"] = df[\"Gold\"].pct_change()\n",
        "    df[\"Gold_Volatility_30\"] = df[\"Gold_Returns\"].rolling(window=30, min_periods=30).std()\n",
        "    df[\"Gold_RSI_14\"] = compute_rsi(df[\"Gold\"], period=14)\n",
        "\n",
        "    # --- Replace any remaining NaN values from rolling windows ---\n",
        "    df = df.ffill().bfill()\n",
        "\n",
        "    # --- Create lagged features for all numeric columns ---\n",
        "    numeric_cols = df.select_dtypes(include=\"number\").columns.tolist()\n",
        "    lags = [1, 3, 5, 7, 10]\n",
        "    df_lagged = add_lag_features(df, numeric_cols, lags)\n",
        "\n",
        "    # --- Drop initial rows with NaN due to lagging ---\n",
        "    df_lagged = df_lagged.dropna()\n",
        "\n",
        "    # --- Define target variable ---\n",
        "    target_col = \"Gold\"\n",
        "\n",
        "    # --- Scale all features (including target) to [0, 1] ---\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_values = scaler.fit_transform(df_lagged.values)\n",
        "    df_scaled = pd.DataFrame(scaled_values, index=df_lagged.index, columns=df_lagged.columns)\n",
        "\n",
        "    # --- Persist preprocessed data ---\n",
        "    df_scaled.to_csv(PREP_CSV_PATH)\n",
        "    log(f\"Preprocessed data saved to {PREP_CSV_PATH}\")\n",
        "\n",
        "    # --- Display sanity outputs ---\n",
        "    display(df.head())\n",
        "    display(df_lagged.head())\n",
        "    display(df_scaled.head())\n",
        "\n",
        "    print(\"Original with indicators shape:\", df.shape)\n",
        "    print(\"Lagged shape:\", df_lagged.shape)\n",
        "    print(\"Scaled shape:\", df_scaled.shape)\n",
        "\n",
        "except Exception as e:\n",
        "    log(f\"Preprocessing failed: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Chronological Split and Sequence Preparation\n",
        "\n",
        "We split the dataset into Train (70%), Validation (15%), and Test (15%) in chronological order. Then we create sequences of length 60 days for the LSTM input and reshape data to `(samples, timesteps, features)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples: 2599\n",
            "Train dates: 2015-11-13 -> 2022-11-02\n",
            "Val dates: 2022-11-03 -> 2024-05-01\n",
            "Test dates: 2024-05-02 -> 2025-10-29\n",
            "X_train: (1759, 60, 84) y_train: (1759,)\n",
            "X_val: (330, 60, 84) y_val: (330,)\n",
            "X_test: (330, 60, 84) y_test: (330,)\n"
          ]
        }
      ],
      "source": [
        "# Chronological split and sequences\n",
        "SEQ_LEN = 60\n",
        "\n",
        "try:\n",
        "    df_scaled = pd.read_csv(PREP_CSV_PATH, index_col=0, parse_dates=True)\n",
        "    target_col = \"Gold\"\n",
        "\n",
        "    # Determine split indices\n",
        "    n = len(df_scaled)\n",
        "    train_end = int(n * 0.70)\n",
        "    val_end = int(n * 0.85)\n",
        "\n",
        "    # Print date ranges\n",
        "    print(\"Total samples:\", n)\n",
        "    print(\"Train dates:\", df_scaled.index[0].date(), \"->\", df_scaled.index[train_end - 1].date())\n",
        "    print(\"Val dates:\", df_scaled.index[train_end].date(), \"->\", df_scaled.index[val_end - 1].date())\n",
        "    print(\"Test dates:\", df_scaled.index[val_end].date(), \"->\", df_scaled.index[-1].date())\n",
        "\n",
        "    # Split data\n",
        "    train_df = df_scaled.iloc[:train_end]\n",
        "    val_df = df_scaled.iloc[train_end:val_end]\n",
        "    test_df = df_scaled.iloc[val_end:]\n",
        "\n",
        "    # Features and targets (predict scaled Gold)\n",
        "    feature_cols = df_scaled.columns.tolist()\n",
        "    X_train_all = train_df[feature_cols].values\n",
        "    y_train_all = train_df[[target_col]].values.squeeze()\n",
        "\n",
        "    X_val_all = val_df[feature_cols].values\n",
        "    y_val_all = val_df[[target_col]].values.squeeze()\n",
        "\n",
        "    X_test_all = test_df[feature_cols].values\n",
        "    y_test_all = test_df[[target_col]].values.squeeze()\n",
        "\n",
        "    # Create sequences\n",
        "    X_train, y_train = create_sequences(X_train_all, y_train_all, SEQ_LEN)\n",
        "    X_val, y_val = create_sequences(X_val_all, y_val_all, SEQ_LEN)\n",
        "    X_test, y_test = create_sequences(X_test_all, y_test_all, SEQ_LEN)\n",
        "\n",
        "    print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
        "    print(\"X_val:\", X_val.shape, \"y_val:\", y_val.shape)\n",
        "    print(\"X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n",
        "except Exception as e:\n",
        "    log(f\"Split/sequencing failed: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Build and Train LSTM Model\n",
        "\n",
        "We use a two-layer LSTM:\n",
        "- LSTM(50, return_sequences=True) + Dropout(0.2)\n",
        "- LSTM(50, return_sequences=False) + Dropout(0.2)\n",
        "- Dense(25, relu)\n",
        "- Output Dense(1)\n",
        "\n",
        "We compile with Adam and MSE loss, and train up to 100 epochs with EarlyStopping and ModelCheckpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">27,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,275</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m27,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m20,200\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)             │         \u001b[38;5;34m1,275\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m26\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">48,501</span> (189.46 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m48,501\u001b[0m (189.46 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">48,501</span> (189.46 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m48,501\u001b[0m (189.46 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-30 22:26:23.869810: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0078\n",
            "Epoch 1: val_loss improved from None to 0.00612, saving model to /Users/keerat/Desktop/Projects/Predict Edge/PredictEdge/gold_price_lstm_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0031 - val_loss: 0.0061\n",
            "Epoch 2/100\n",
            "\u001b[1m54/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.1954e-04\n",
            "Epoch 2: val_loss did not improve from 0.00612\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 7.3381e-04 - val_loss: 0.0076\n",
            "Epoch 3/100\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 5.5016e-04\n",
            "Epoch 3: val_loss did not improve from 0.00612\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 5.2366e-04 - val_loss: 0.0079\n",
            "Epoch 4/100\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 5.0544e-04\n",
            "Epoch 4: val_loss did not improve from 0.00612\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 4.6389e-04 - val_loss: 0.0085\n",
            "Epoch 5/100\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 4.7562e-04\n",
            "Epoch 5: val_loss did not improve from 0.00612\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 4.2252e-04 - val_loss: 0.0066\n",
            "Epoch 6/100\n",
            "\u001b[1m53/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 3.6457e-04\n",
            "Epoch 6: val_loss improved from 0.00612 to 0.00525, saving model to /Users/keerat/Desktop/Projects/Predict Edge/PredictEdge/gold_price_lstm_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 3.4466e-04 - val_loss: 0.0053\n",
            "Epoch 7/100\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.8118e-04\n",
            "Epoch 7: val_loss improved from 0.00525 to 0.00457, saving model to /Users/keerat/Desktop/Projects/Predict Edge/PredictEdge/gold_price_lstm_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 2.8858e-04 - val_loss: 0.0046\n",
            "Epoch 8/100\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 3.0534e-04\n",
            "Epoch 8: val_loss did not improve from 0.00457\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 2.9516e-04 - val_loss: 0.0068\n",
            "Epoch 9/100\n",
            "\u001b[1m54/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.6490e-04\n",
            "Epoch 9: val_loss did not improve from 0.00457\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 2.5816e-04 - val_loss: 0.0054\n",
            "Epoch 10/100\n",
            "\u001b[1m54/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.7535e-04\n",
            "Epoch 10: val_loss improved from 0.00457 to 0.00427, saving model to /Users/keerat/Desktop/Projects/Predict Edge/PredictEdge/gold_price_lstm_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 2.5225e-04 - val_loss: 0.0043\n",
            "Epoch 11/100\n",
            "\u001b[1m54/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.1491e-04\n",
            "Epoch 11: val_loss did not improve from 0.00427\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 2.2136e-04 - val_loss: 0.0052\n",
            "Epoch 12/100\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.1680e-04\n",
            "Epoch 12: val_loss did not improve from 0.00427\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 2.1232e-04 - val_loss: 0.0052\n",
            "Epoch 13/100\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.0154e-04\n",
            "Epoch 13: val_loss did not improve from 0.00427\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 2.0908e-04 - val_loss: 0.0050\n",
            "Epoch 14/100\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.7691e-04\n",
            "Epoch 14: val_loss did not improve from 0.00427\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 1.8096e-04 - val_loss: 0.0047\n",
            "Epoch 15/100\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.0270e-04\n",
            "Epoch 15: val_loss improved from 0.00427 to 0.00416, saving model to /Users/keerat/Desktop/Projects/Predict Edge/PredictEdge/gold_price_lstm_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 1.9567e-04 - val_loss: 0.0042\n",
            "Epoch 16/100\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.8063e-04\n",
            "Epoch 16: val_loss improved from 0.00416 to 0.00325, saving model to /Users/keerat/Desktop/Projects/Predict Edge/PredictEdge/gold_price_lstm_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 1.7896e-04 - val_loss: 0.0033\n",
            "Epoch 17/100\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.6034e-04\n",
            "Epoch 17: val_loss did not improve from 0.00325\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 1.6313e-04 - val_loss: 0.0045\n",
            "Epoch 18/100\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.8525e-04\n",
            "Epoch 18: val_loss did not improve from 0.00325\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 1.8832e-04 - val_loss: 0.0034\n",
            "Epoch 19/100\n",
            "\u001b[1m53/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.7213e-04\n",
            "Epoch 19: val_loss did not improve from 0.00325\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 1.5818e-04 - val_loss: 0.0035\n",
            "Epoch 20/100\n",
            "\u001b[1m53/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.4853e-04\n",
            "Epoch 20: val_loss did not improve from 0.00325\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 1.5337e-04 - val_loss: 0.0037\n",
            "Epoch 21/100\n",
            "\u001b[1m54/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.7264e-04\n",
            "Epoch 21: val_loss did not improve from 0.00325\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 1.5719e-04 - val_loss: 0.0034\n",
            "Epoch 22/100\n",
            "\u001b[1m54/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.5892e-04\n",
            "Epoch 22: val_loss improved from 0.00325 to 0.00271, saving model to /Users/keerat/Desktop/Projects/Predict Edge/PredictEdge/gold_price_lstm_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 1.6029e-04 - val_loss: 0.0027\n",
            "Epoch 23/100\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.4217e-04\n",
            "Epoch 23: val_loss did not improve from 0.00271\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 1.4945e-04 - val_loss: 0.0031\n",
            "Epoch 24/100\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.3628e-04\n",
            "Epoch 24: val_loss improved from 0.00271 to 0.00240, saving model to /Users/keerat/Desktop/Projects/Predict Edge/PredictEdge/gold_price_lstm_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 1.4217e-04 - val_loss: 0.0024\n",
            "Epoch 25/100\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.2728e-04\n",
            "Epoch 25: val_loss improved from 0.00240 to 0.00211, saving model to /Users/keerat/Desktop/Projects/Predict Edge/PredictEdge/gold_price_lstm_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 1.3101e-04 - val_loss: 0.0021\n",
            "Epoch 26/100\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.1707e-04\n",
            "Epoch 26: val_loss did not improve from 0.00211\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 1.2610e-04 - val_loss: 0.0035\n",
            "Epoch 27/100\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.5216e-04\n",
            "Epoch 27: val_loss did not improve from 0.00211\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 1.3829e-04 - val_loss: 0.0034\n",
            "Epoch 28/100\n",
            "\u001b[1m54/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.4131e-04\n",
            "Epoch 28: val_loss did not improve from 0.00211\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 1.3130e-04 - val_loss: 0.0030\n",
            "Epoch 29/100\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.2152e-04\n",
            "Epoch 29: val_loss did not improve from 0.00211\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 1.2235e-04 - val_loss: 0.0028\n",
            "Epoch 30/100\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.1745e-04\n",
            "Epoch 30: val_loss did not improve from 0.00211\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 1.1513e-04 - val_loss: 0.0030\n",
            "Epoch 31/100\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.2114e-04\n",
            "Epoch 31: val_loss did not improve from 0.00211\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 1.2260e-04 - val_loss: 0.0035\n",
            "Epoch 32/100\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.3987e-04\n",
            "Epoch 32: val_loss did not improve from 0.00211\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 1.2491e-04 - val_loss: 0.0030\n",
            "Epoch 33/100\n",
            "\u001b[1m54/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.1829e-04\n",
            "Epoch 33: val_loss did not improve from 0.00211\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 1.1813e-04 - val_loss: 0.0028\n",
            "Epoch 34/100\n",
            "\u001b[1m53/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.2374e-04\n",
            "Epoch 34: val_loss did not improve from 0.00211\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 1.1212e-04 - val_loss: 0.0024\n",
            "Epoch 35/100\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.2219e-04\n",
            "Epoch 35: val_loss did not improve from 0.00211\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 1.2950e-04 - val_loss: 0.0034\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Model saved to /Users/keerat/Desktop/Projects/Predict Edge/PredictEdge/gold_price_lstm_model.h5\n"
          ]
        }
      ],
      "source": [
        "# Build and train the LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "try:\n",
        "    model = Sequential([\n",
        "        LSTM(50, return_sequences=True, input_shape=(SEQ_LEN, X_train.shape[-1])),\n",
        "        Dropout(0.2),\n",
        "        LSTM(50, return_sequences=False),\n",
        "        Dropout(0.2),\n",
        "        Dense(25, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    model.summary()\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "        ModelCheckpoint(MODEL_PATH, monitor='val_loss', save_best_only=True, verbose=1)\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Ensure best model saved\n",
        "    model.save(MODEL_PATH)\n",
        "    log(f\"Model saved to {MODEL_PATH}\")\n",
        "except Exception as e:\n",
        "    log(f\"Model training failed: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Evaluation and Inverse Transform\n",
        "\n",
        "We evaluate on train, validation, and test sets. We inverse-transform the predictions and targets back to the original scale for metric calculation: RMSE, MAE, MAPE, and R².\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using 'Gold_Gold' as the target column for scaling and evaluation.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-30 22:39:59.312005: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
            "\n",
            "Evaluation Metrics:\n",
            "               RMSE      MAE    MAPE      R2\n",
            "Train       25.9106  19.4883  1.2796  0.9902\n",
            "Validation 150.7725 132.8804  6.4972 -0.5807\n",
            "Test       868.7777 792.3408 24.9020 -2.7443\n"
          ]
        }
      ],
      "source": [
        "# # Evaluate model and inverse transform\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "try:\n",
        "    # --- Identify the correct Gold column dynamically ---\n",
        "    gold_col = [c for c in data.columns if \"Gold\" in c][0]\n",
        "    print(f\"Using '{gold_col}' as the target column for scaling and evaluation.\")\n",
        "\n",
        "    # --- Reconstruct lagged features for the scaler ---\n",
        "    original_df_lagged = add_lag_features(data, data.columns.tolist(), [1, 3, 5, 7, 10]).dropna()\n",
        "\n",
        "    # --- Ensure gold column exists in lagged DataFrame ---\n",
        "    if gold_col not in original_df_lagged.columns:\n",
        "        original_df_lagged[gold_col] = data[gold_col].iloc[-len(original_df_lagged):].values\n",
        "\n",
        "    # --- Fit target scaler ---\n",
        "    target_scaler = MinMaxScaler()\n",
        "    target_scaler.fit(original_df_lagged[[gold_col]].values)\n",
        "\n",
        "    # --- Load trained model ---\n",
        "    best_model = load_model(MODEL_PATH)\n",
        "\n",
        "    # --- Predict ---\n",
        "    y_train_pred = best_model.predict(X_train).squeeze()\n",
        "    y_val_pred = best_model.predict(X_val).squeeze()\n",
        "    y_test_pred = best_model.predict(X_test).squeeze()\n",
        "\n",
        "    # --- Inverse transform predictions and true values ---\n",
        "    y_train_true_inv = target_scaler.inverse_transform(y_train.reshape(-1, 1)).squeeze()\n",
        "    y_val_true_inv = target_scaler.inverse_transform(y_val.reshape(-1, 1)).squeeze()\n",
        "    y_test_true_inv = target_scaler.inverse_transform(y_test.reshape(-1, 1)).squeeze()\n",
        "\n",
        "    y_train_pred_inv = target_scaler.inverse_transform(y_train_pred.reshape(-1, 1)).squeeze()\n",
        "    y_val_pred_inv = target_scaler.inverse_transform(y_val_pred.reshape(-1, 1)).squeeze()\n",
        "    y_test_pred_inv = target_scaler.inverse_transform(y_test_pred.reshape(-1, 1)).squeeze()\n",
        "\n",
        "    # --- Compute metrics ---\n",
        "    results = {\n",
        "        \"Train\": {\n",
        "            \"RMSE\": math.sqrt(mean_squared_error(y_train_true_inv, y_train_pred_inv)),\n",
        "            \"MAE\": mean_absolute_error(y_train_true_inv, y_train_pred_inv),\n",
        "            \"MAPE\": mape(y_train_true_inv, y_train_pred_inv),\n",
        "            \"R2\": r2_score(y_train_true_inv, y_train_pred_inv),\n",
        "        },\n",
        "        \"Validation\": {\n",
        "            \"RMSE\": math.sqrt(mean_squared_error(y_val_true_inv, y_val_pred_inv)),\n",
        "            \"MAE\": mean_absolute_error(y_val_true_inv, y_val_pred_inv),\n",
        "            \"MAPE\": mape(y_val_true_inv, y_val_pred_inv),\n",
        "            \"R2\": r2_score(y_val_true_inv, y_val_pred_inv),\n",
        "        },\n",
        "        \"Test\": {\n",
        "            \"RMSE\": math.sqrt(mean_squared_error(y_test_true_inv, y_test_pred_inv)),\n",
        "            \"MAE\": mean_absolute_error(y_test_true_inv, y_test_pred_inv),\n",
        "            \"MAPE\": mape(y_test_true_inv, y_test_pred_inv),\n",
        "            \"R2\": r2_score(y_test_true_inv, y_test_pred_inv),\n",
        "        },\n",
        "    }\n",
        "\n",
        "    # --- Print metrics ---\n",
        "    print_metrics_table(results)\n",
        "\n",
        "except Exception as e:\n",
        "    log(f\"Evaluation failed: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Visualization\n",
        "\n",
        "We produce:\n",
        "1. Training history (loss over epochs)\n",
        "2. Actual vs Predicted (train, val, test)\n",
        "3. Prediction error distribution\n",
        "4. Feature correlation heatmap\n",
        "5. Last 100 days actual vs predicted with confidence bounds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Visualization failed: Mime type rendering requires nbformat>=4.2.0 but it is not installed\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m fig.add_trace(go.Scatter(x=test_dates, y=y_test_pred_inv, mode=\u001b[33m'\u001b[39m\u001b[33mlines\u001b[39m\u001b[33m'\u001b[39m, name=\u001b[33m'\u001b[39m\u001b[33mTest Pred\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     29\u001b[39m fig.update_layout(title=\u001b[33m'\u001b[39m\u001b[33mActual vs Predicted Gold Prices\u001b[39m\u001b[33m'\u001b[39m, xaxis_title=\u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m, yaxis_title=\u001b[33m'\u001b[39m\u001b[33mPrice\u001b[39m\u001b[33m'\u001b[39m, template=\u001b[33m'\u001b[39m\u001b[33mplotly_white\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# 3) Prediction error distribution (test)\u001b[39;00m\n\u001b[32m     33\u001b[39m errors = y_test_true_inv - y_test_pred_inv\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/plotly/basedatatypes.py:3420\u001b[39m, in \u001b[36mBaseFigure.show\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3387\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3388\u001b[39m \u001b[33;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[32m   3389\u001b[39m \u001b[33;03mspecified by the renderer argument\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3416\u001b[39m \u001b[33;03mNone\u001b[39;00m\n\u001b[32m   3417\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3418\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpio\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3420\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/plotly/io/_renderers.py:415\u001b[39m, in \u001b[36mshow\u001b[39m\u001b[34m(fig, renderer, validate, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    411\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    412\u001b[39m     )\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat.__version__) < Version(\u001b[33m\"\u001b[39m\u001b[33m4.2.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    416\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m     )\n\u001b[32m    419\u001b[39m display_jupyter_version_warnings()\n\u001b[32m    421\u001b[39m ipython_display.display(bundle, raw=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[31mValueError\u001b[39m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
          ]
        }
      ],
      "source": [
        "# Visualizations\n",
        "try:\n",
        "    # 1) Training history\n",
        "    hist_df = pd.DataFrame(history.history)\n",
        "    fig, ax = plt.subplots()\n",
        "    hist_df[[\"loss\", \"val_loss\"]].plot(ax=ax)\n",
        "    ax.set_title(\"Training History: Loss over Epochs\")\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"MSE Loss\")\n",
        "    ax.legend([\"Train Loss\", \"Val Loss\"])\n",
        "    plt.show()\n",
        "\n",
        "    # 2) Actual vs Predicted\n",
        "    # Reconstruct aligned dates for sequences\n",
        "    train_dates = train_df.index[SEQ_LEN:]\n",
        "    val_dates = val_df.index[SEQ_LEN:]\n",
        "    test_dates = test_df.index[SEQ_LEN:]\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=train_dates, y=y_train_true_inv, mode='lines', name='Train Actual'))\n",
        "    fig.add_trace(go.Scatter(x=train_dates, y=y_train_pred_inv, mode='lines', name='Train Pred'))\n",
        "\n",
        "    fig.add_trace(go.Scatter(x=val_dates, y=y_val_true_inv, mode='lines', name='Val Actual'))\n",
        "    fig.add_trace(go.Scatter(x=val_dates, y=y_val_pred_inv, mode='lines', name='Val Pred'))\n",
        "\n",
        "    fig.add_trace(go.Scatter(x=test_dates, y=y_test_true_inv, mode='lines', name='Test Actual'))\n",
        "    fig.add_trace(go.Scatter(x=test_dates, y=y_test_pred_inv, mode='lines', name='Test Pred'))\n",
        "\n",
        "    fig.update_layout(title='Actual vs Predicted Gold Prices', xaxis_title='Date', yaxis_title='Price', template='plotly_white')\n",
        "    fig.show()\n",
        "\n",
        "    # 3) Prediction error distribution (test)\n",
        "    errors = y_test_true_inv - y_test_pred_inv\n",
        "    plt.figure(figsize=(10,6))\n",
        "    sns.histplot(errors, bins=50, kde=True)\n",
        "    plt.title(\"Prediction Error Distribution (Test)\")\n",
        "    plt.xlabel(\"Error (Actual - Pred)\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n",
        "\n",
        "    # 4) Feature correlation heatmap (on scaled features)\n",
        "    corr = df_scaled.corr()\n",
        "    plt.figure(figsize=(14,10))\n",
        "    sns.heatmap(corr, cmap='coolwarm', center=0, cbar_kws={'shrink': .6})\n",
        "    plt.title(\"Feature Correlation Heatmap (Scaled)\")\n",
        "    plt.show()\n",
        "\n",
        "    # 5) Last 100 days with confidence bounds (using residual std)\n",
        "    last_n = 100\n",
        "    last_dates = test_dates[-last_n:]\n",
        "    last_true = y_test_true_inv[-last_n:]\n",
        "    last_pred = y_test_pred_inv[-last_n:]\n",
        "    resid_std = np.std(errors)\n",
        "    upper = last_pred + 1.96 * resid_std\n",
        "    lower = last_pred - 1.96 * resid_std\n",
        "\n",
        "    fig2 = go.Figure()\n",
        "    fig2.add_trace(go.Scatter(x=last_dates, y=last_true, mode='lines', name='Actual'))\n",
        "    fig2.add_trace(go.Scatter(x=last_dates, y=last_pred, mode='lines', name='Predicted'))\n",
        "    fig2.add_trace(go.Scatter(x=last_dates, y=upper, mode='lines', name='Upper 95%', line=dict(dash='dash')))\n",
        "    fig2.add_trace(go.Scatter(x=last_dates, y=lower, mode='lines', name='Lower 95%', line=dict(dash='dash')))\n",
        "    fig2.update_layout(title='Last 100 Days: Prediction with Confidence Bounds', xaxis_title='Date', yaxis_title='Price', template='plotly_white')\n",
        "    fig2.show()\n",
        "except Exception as e:\n",
        "    log(f\"Visualization failed: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Future Predictions (Next 7 Days)\n",
        "\n",
        "We use the last 60 days of scaled features to iteratively predict the next 7 days. Predictions are inverse-transformed to the original price scale and displayed with dates and a plot for context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Future prediction failed: \"None of [Index(['Gold'], dtype='object')] are in the [columns]\"\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "\"None of [Index(['Gold'], dtype='object')] are in the [columns]\"",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m original_df_lagged = add_lag_features(data, data.columns.tolist(), [\u001b[32m1\u001b[39m,\u001b[32m3\u001b[39m,\u001b[32m5\u001b[39m,\u001b[32m7\u001b[39m,\u001b[32m10\u001b[39m]).dropna()\n\u001b[32m      8\u001b[39m target_scaler = MinMaxScaler()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m target_scaler.fit(\u001b[43moriginal_df_lagged\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGold\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Use last SEQ_LEN rows from the full scaled dataset\u001b[39;00m\n\u001b[32m     12\u001b[39m recent_window = full_scaled.values[-SEQ_LEN:]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/base.py:6261\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6260\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6261\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mKeyError\u001b[39m: \"None of [Index(['Gold'], dtype='object')] are in the [columns]\""
          ]
        }
      ],
      "source": [
        "# Future predictions (7 days)\n",
        "try:\n",
        "    # We'll build from the most recent scaled frame\n",
        "    full_scaled = df_scaled.copy()\n",
        "\n",
        "    # Prepare scalers for inverse transform of target\n",
        "    original_df_lagged = add_lag_features(data, data.columns.tolist(), [1,3,5,7,10]).dropna()\n",
        "    target_scaler = MinMaxScaler()\n",
        "    target_scaler.fit(original_df_lagged[[\"Gold\"]])\n",
        "\n",
        "    # Use last SEQ_LEN rows from the full scaled dataset\n",
        "    recent_window = full_scaled.values[-SEQ_LEN:]\n",
        "\n",
        "    future_scaled_preds = []\n",
        "    future_dates = []\n",
        "\n",
        "    # We only predict the target (Gold) one step ahead each iteration, without updating other features\n",
        "    # For a production system, consider a full-feature recursive strategy.\n",
        "    current_window = recent_window.copy()\n",
        "    for i in range(7):\n",
        "        x_input = current_window.reshape(1, SEQ_LEN, full_scaled.shape[1])\n",
        "        next_scaled = best_model.predict(x_input).squeeze()\n",
        "        future_scaled_preds.append(next_scaled)\n",
        "        next_date = (df_scaled.index[-1] + pd.Timedelta(days=1+i))\n",
        "        # Move window forward by appending a vector where only target is updated\n",
        "        next_row = current_window[-1].copy()\n",
        "        # Target column index\n",
        "        target_idx = list(full_scaled.columns).index(\"Gold\")\n",
        "        next_row[target_idx] = next_scaled\n",
        "        current_window = np.vstack([current_window[1:], next_row])\n",
        "        future_dates.append(next_date)\n",
        "\n",
        "    # Inverse transform predictions\n",
        "    future_preds_inv = target_scaler.inverse_transform(np.array(future_scaled_preds).reshape(-1, 1)).squeeze()\n",
        "\n",
        "    # Display table\n",
        "    future_df = pd.DataFrame({\"Date\": pd.to_datetime(future_dates), \"Predicted_Gold_Price\": future_preds_inv})\n",
        "    display(future_df)\n",
        "\n",
        "    # Plot with historical context\n",
        "    history_days = 200\n",
        "    context_dates = test_dates[-history_days:]\n",
        "    context_true = y_test_true_inv[-history_days:]\n",
        "\n",
        "    fig3 = go.Figure()\n",
        "    fig3.add_trace(go.Scatter(x=context_dates, y=context_true, mode='lines', name='Recent Actual'))\n",
        "    fig3.add_trace(go.Scatter(x=future_df[\"Date\"], y=future_df[\"Predicted_Gold_Price\"], mode='lines+markers', name='Future Pred (7d)'))\n",
        "    fig3.update_layout(title='Future 7-Day Forecast with Historical Context', xaxis_title='Date', yaxis_title='Price', template='plotly_white')\n",
        "    fig3.show()\n",
        "except Exception as e:\n",
        "    log(f\"Future prediction failed: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "In this notebook, we built a robust, end-to-end LSTM pipeline for gold price prediction, including data collection, preprocessing, modeling, evaluation, visualization, and short-term forecasting. Consider improving the model by:\n",
        "\n",
        "- Adding exogenous feature engineering (macro surprises, term spreads, commodity spreads)\n",
        "- Testing alternative sequence lengths, architectures (GRU, TCN), and hyperparameters\n",
        "- Using walk-forward validation for more realistic evaluation\n",
        "- Calibrating prediction intervals with quantile models or bootstrapping\n",
        "- Incorporating regime detection (volatility states) for conditional modeling\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
