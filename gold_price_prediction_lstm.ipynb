{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gold Price Prediction with LSTM (End-to-End)\n",
        "\n",
        "This notebook builds a complete, production-ready pipeline to predict daily gold prices using Long Short-Term Memory (LSTM) neural networks. It includes:\n",
        "\n",
        "- Data collection from `yfinance` and `FRED` (via `pandas_datareader`)\n",
        "- Feature engineering and preprocessing\n",
        "- Chronological train/validation/test splitting\n",
        "- LSTM modeling with callbacks and model persistence\n",
        "- Evaluation with multiple metrics and publication-ready visualizations\n",
        "- 7-day future prediction with uncertainty bounds\n",
        "\n",
        "We follow best practices for time series modeling and include robust error handling and fallbacks where necessary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Environment initialized.\n"
          ]
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# ## Setup and Imports\n",
        "# - Set random seeds for reproducibility\n",
        "# - Import required libraries\n",
        "# - Configure plotting styles\n",
        "# - Define utility functions used throughout the notebook\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Print progress helper\n",
        "def log(msg: str) -> None:\n",
        "    print(f\"[INFO] {msg}\")\n",
        "\n",
        "# Reproducibility\n",
        "import random\n",
        "import numpy as np\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    tf.keras.utils.set_random_seed(42)\n",
        "    tf.config.experimental.enable_op_determinism()\n",
        "except Exception as e:\n",
        "    log(\"TensorFlow determinism not fully enabled; continuing.\")\n",
        "\n",
        "# Core libs\n",
        "import pandas as pd\n",
        "from pandas import DataFrame, Series\n",
        "\n",
        "# Data sources\n",
        "try:\n",
        "    import yfinance as yf\n",
        "except Exception as e:\n",
        "    raise ImportError(\"Please install yfinance: pip install yfinance\")\n",
        "\n",
        "try:\n",
        "    from pandas_datareader import data as pdr\n",
        "except Exception:\n",
        "    raise ImportError(\"Please install pandas_datareader: pip install pandas_datareader\")\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "plt.rcParams[\"axes.grid\"] = True\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# Paths\n",
        "PROJECT_ROOT = \"/Users/keerat/Desktop/Projects/Predict Edge/PredictEdge\"\n",
        "RAW_CSV_PATH = os.path.join(PROJECT_ROOT, \"gold_raw_data.csv\")\n",
        "PREP_CSV_PATH = os.path.join(PROJECT_ROOT, \"gold_preprocessed_data.csv\")\n",
        "MODEL_PATH = os.path.join(PROJECT_ROOT, \"gold_price_lstm_model.h5\")\n",
        "\n",
        "log(\"Environment initialized.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Utility functions loaded.\n"
          ]
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# ### Utility Functions\n",
        "# Helper utilities for downloads, indicators, lag creation, sequences, and metrics.\n",
        "\n",
        "from typing import Tuple, List, Optional, Dict\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# ----------------------------------------\n",
        "# Download helpers with retries and fallbacks\n",
        "# ----------------------------------------\n",
        "def download_yfinance(ticker: str, start: str, end: str, interval: str = \"1d\", max_retries: int = 3) -> DataFrame:\n",
        "    \"\"\"Download data from yfinance with basic retries.\n",
        "    Returns a DataFrame with a DatetimeIndex. If download fails, returns synthetic series.\n",
        "    \"\"\"\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            log(f\"Downloading {ticker} from yfinance (attempt {attempt})...\")\n",
        "            df = yf.download(ticker, start=start, end=end, interval=interval, auto_adjust=True, progress=False)\n",
        "            if not df.empty:\n",
        "                df = df.rename(columns={\"Close\": ticker}).loc[:, [ticker]]\n",
        "                return df\n",
        "        except Exception as e:\n",
        "            log(f\"Download error for {ticker}: {e}\")\n",
        "    # Synthetic fallback (random walk)\n",
        "    log(f\"Falling back to synthetic data for {ticker}.\")\n",
        "    dates = pd.date_range(start=start, end=end, freq=\"B\")\n",
        "    rnd = np.random.default_rng(42)\n",
        "    steps = rnd.normal(0, 1, size=len(dates))\n",
        "    series = 100 + np.cumsum(steps)\n",
        "    return pd.DataFrame(series, index=dates, columns=[ticker])\n",
        "\n",
        "\n",
        "def download_fred(series_id: str, start: str, end: str, max_retries: int = 3) -> DataFrame:\n",
        "    \"\"\"Download data from FRED via pandas_datareader. Falls back to synthetic if needed.\"\"\"\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            log(f\"Downloading {series_id} from FRED (attempt {attempt})...\")\n",
        "            df = pdr.DataReader(series_id, \"fred\", start, end)\n",
        "            if not df.empty:\n",
        "                df = df.rename(columns={series_id: series_id})\n",
        "                return df\n",
        "        except Exception as e:\n",
        "            log(f\"FRED download error for {series_id}: {e}\")\n",
        "    # Synthetic monthly CPI-like trend\n",
        "    log(f\"Falling back to synthetic data for {series_id}.\")\n",
        "    dates = pd.date_range(start=start, end=end, freq=\"MS\")\n",
        "    trend = np.linspace(250, 300, num=len(dates))\n",
        "    noise = np.random.default_rng(42).normal(0, 0.3, size=len(dates))\n",
        "    series = trend + noise\n",
        "    return pd.DataFrame(series, index=dates, columns=[series_id])\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# Indicators and feature engineering\n",
        "# ----------------------------------------\n",
        "def compute_rsi(series: Series, period: int = 14) -> Series:\n",
        "    \"\"\"Compute Relative Strength Index (RSI).\"\"\"\n",
        "    delta = series.diff()\n",
        "    gain = delta.clip(lower=0.0)\n",
        "    loss = -delta.clip(upper=0.0)\n",
        "    avg_gain = gain.rolling(window=period, min_periods=period).mean()\n",
        "    avg_loss = loss.rolling(window=period, min_periods=period).mean()\n",
        "    rs = avg_gain / (avg_loss.replace(0, np.nan))\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi.fillna(method=\"bfill\").fillna(50.0)\n",
        "\n",
        "\n",
        "def add_lag_features(df: DataFrame, columns: List[str], lags: List[int]) -> DataFrame:\n",
        "    \"\"\"Create lag features for given columns and lags.\"\"\"\n",
        "    out = df.copy()\n",
        "    for col in columns:\n",
        "        for l in lags:\n",
        "            out[f\"{col}_lag_{l}\"] = out[col].shift(l)\n",
        "    return out\n",
        "\n",
        "\n",
        "def create_sequences(features: np.ndarray, targets: np.ndarray, seq_len: int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Create LSTM sequences of length seq_len from aligned feature and target arrays.\"\"\"\n",
        "    X_list, y_list = [], []\n",
        "    for i in range(seq_len, len(features)):\n",
        "        X_list.append(features[i - seq_len:i])\n",
        "        y_list.append(targets[i])\n",
        "    return np.array(X_list), np.array(y_list)\n",
        "\n",
        "\n",
        "def mape(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    y_true = np.array(y_true).astype(float)\n",
        "    y_pred = np.array(y_pred).astype(float)\n",
        "    nonzero = np.where(y_true != 0)[0]\n",
        "    if len(nonzero) == 0:\n",
        "        return np.nan\n",
        "    return np.mean(np.abs((y_true[nonzero] - y_pred[nonzero]) / y_true[nonzero])) * 100\n",
        "\n",
        "\n",
        "def print_metrics_table(results: Dict[str, Dict[str, float]]) -> None:\n",
        "    \"\"\"Print a formatted metrics table for train/val/test.\"\"\"\n",
        "    df = pd.DataFrame(results).T[[\"RMSE\", \"MAE\", \"MAPE\", \"R2\"]]\n",
        "    print(\"\\nEvaluation Metrics:\")\n",
        "    print(df.to_string(float_format=lambda x: f\"{x:,.4f}\"))\n",
        "\n",
        "\n",
        "log(\"Utility functions loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Data Collection\n",
        "\n",
        "We will download 10 years of historical data for:\n",
        "- Gold Futures (`GC=F`) as the target series\n",
        "- Economic indicators: `^TNX`, `DX-Y.NYB`, `^GSPC`, `^VIX`, `CL=F`, `SI=F`\n",
        "- CPI from FRED (`CPIAUCSL`)\n",
        "\n",
        "All datasets will be merged on date and forward-filled to handle missing values. If any API fails, we will fallback to synthetic data to keep the pipeline runnable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Downloading GC=F from yfinance (attempt 1)...\n",
            "[INFO] Downloading ^TNX from yfinance (attempt 1)...\n",
            "[INFO] Downloading DX-Y.NYB from yfinance (attempt 1)...\n",
            "[INFO] Downloading ^GSPC from yfinance (attempt 1)...\n",
            "[INFO] Downloading ^VIX from yfinance (attempt 1)...\n",
            "[INFO] Downloading CL=F from yfinance (attempt 1)...\n",
            "[INFO] Downloading SI=F from yfinance (attempt 1)...\n",
            "[INFO] Downloading CPIAUCSL from FRED (attempt 1)...\n",
            "[INFO] Raw data saved to /Users/keerat/Desktop/Projects/Predict Edge/PredictEdge/gold_raw_data.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gold_Gold</th>\n",
              "      <th>TNX_10Y_TNX_10Y</th>\n",
              "      <th>DXY_DXY</th>\n",
              "      <th>SP500_SP500</th>\n",
              "      <th>VIX_VIX</th>\n",
              "      <th>CrudeOil_CrudeOil</th>\n",
              "      <th>Silver_Silver</th>\n",
              "      <th>CPIAUCSL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2015-10-30</th>\n",
              "      <td>1141.500000</td>\n",
              "      <td>2.151</td>\n",
              "      <td>96.949997</td>\n",
              "      <td>2079.360107</td>\n",
              "      <td>15.07</td>\n",
              "      <td>46.590000</td>\n",
              "      <td>15.566</td>\n",
              "      <td>238.017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-11-02</th>\n",
              "      <td>1135.800049</td>\n",
              "      <td>2.187</td>\n",
              "      <td>96.930000</td>\n",
              "      <td>2104.050049</td>\n",
              "      <td>14.15</td>\n",
              "      <td>46.139999</td>\n",
              "      <td>15.413</td>\n",
              "      <td>238.017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-11-03</th>\n",
              "      <td>1114.199951</td>\n",
              "      <td>2.220</td>\n",
              "      <td>97.160004</td>\n",
              "      <td>2109.790039</td>\n",
              "      <td>14.54</td>\n",
              "      <td>47.900002</td>\n",
              "      <td>15.244</td>\n",
              "      <td>238.017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-11-04</th>\n",
              "      <td>1106.500000</td>\n",
              "      <td>2.230</td>\n",
              "      <td>97.949997</td>\n",
              "      <td>2102.310059</td>\n",
              "      <td>15.51</td>\n",
              "      <td>46.320000</td>\n",
              "      <td>15.063</td>\n",
              "      <td>238.017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-11-05</th>\n",
              "      <td>1104.400024</td>\n",
              "      <td>2.245</td>\n",
              "      <td>97.940002</td>\n",
              "      <td>2099.929932</td>\n",
              "      <td>15.05</td>\n",
              "      <td>45.200001</td>\n",
              "      <td>14.988</td>\n",
              "      <td>238.017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Gold_Gold  TNX_10Y_TNX_10Y    DXY_DXY  SP500_SP500  VIX_VIX  \\\n",
              "2015-10-30  1141.500000            2.151  96.949997  2079.360107    15.07   \n",
              "2015-11-02  1135.800049            2.187  96.930000  2104.050049    14.15   \n",
              "2015-11-03  1114.199951            2.220  97.160004  2109.790039    14.54   \n",
              "2015-11-04  1106.500000            2.230  97.949997  2102.310059    15.51   \n",
              "2015-11-05  1104.400024            2.245  97.940002  2099.929932    15.05   \n",
              "\n",
              "            CrudeOil_CrudeOil  Silver_Silver  CPIAUCSL  \n",
              "2015-10-30          46.590000         15.566   238.017  \n",
              "2015-11-02          46.139999         15.413   238.017  \n",
              "2015-11-03          47.900002         15.244   238.017  \n",
              "2015-11-04          46.320000         15.063   238.017  \n",
              "2015-11-05          45.200001         14.988   238.017  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gold_Gold</th>\n",
              "      <th>TNX_10Y_TNX_10Y</th>\n",
              "      <th>DXY_DXY</th>\n",
              "      <th>SP500_SP500</th>\n",
              "      <th>VIX_VIX</th>\n",
              "      <th>CrudeOil_CrudeOil</th>\n",
              "      <th>Silver_Silver</th>\n",
              "      <th>CPIAUCSL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2025-10-23</th>\n",
              "      <td>4125.500000</td>\n",
              "      <td>3.991</td>\n",
              "      <td>98.940002</td>\n",
              "      <td>6738.439941</td>\n",
              "      <td>17.299999</td>\n",
              "      <td>61.790001</td>\n",
              "      <td>48.481998</td>\n",
              "      <td>324.368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-24</th>\n",
              "      <td>4118.399902</td>\n",
              "      <td>3.997</td>\n",
              "      <td>98.949997</td>\n",
              "      <td>6791.689941</td>\n",
              "      <td>16.370001</td>\n",
              "      <td>61.500000</td>\n",
              "      <td>48.376999</td>\n",
              "      <td>324.368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-27</th>\n",
              "      <td>4001.899902</td>\n",
              "      <td>3.997</td>\n",
              "      <td>98.779999</td>\n",
              "      <td>6875.160156</td>\n",
              "      <td>15.790000</td>\n",
              "      <td>61.310001</td>\n",
              "      <td>46.562000</td>\n",
              "      <td>324.368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-28</th>\n",
              "      <td>3966.199951</td>\n",
              "      <td>3.983</td>\n",
              "      <td>98.690002</td>\n",
              "      <td>6890.890137</td>\n",
              "      <td>16.420000</td>\n",
              "      <td>60.150002</td>\n",
              "      <td>47.125000</td>\n",
              "      <td>324.368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-29</th>\n",
              "      <td>3983.699951</td>\n",
              "      <td>4.058</td>\n",
              "      <td>99.220001</td>\n",
              "      <td>6890.589844</td>\n",
              "      <td>16.920000</td>\n",
              "      <td>60.480000</td>\n",
              "      <td>47.721001</td>\n",
              "      <td>324.368</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Gold_Gold  TNX_10Y_TNX_10Y    DXY_DXY  SP500_SP500    VIX_VIX  \\\n",
              "2025-10-23  4125.500000            3.991  98.940002  6738.439941  17.299999   \n",
              "2025-10-24  4118.399902            3.997  98.949997  6791.689941  16.370001   \n",
              "2025-10-27  4001.899902            3.997  98.779999  6875.160156  15.790000   \n",
              "2025-10-28  3966.199951            3.983  98.690002  6890.890137  16.420000   \n",
              "2025-10-29  3983.699951            4.058  99.220001  6890.589844  16.920000   \n",
              "\n",
              "            CrudeOil_CrudeOil  Silver_Silver  CPIAUCSL  \n",
              "2025-10-23          61.790001      48.481998   324.368  \n",
              "2025-10-24          61.500000      48.376999   324.368  \n",
              "2025-10-27          61.310001      46.562000   324.368  \n",
              "2025-10-28          60.150002      47.125000   324.368  \n",
              "2025-10-29          60.480000      47.721001   324.368  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape: (2609, 8)\n"
          ]
        }
      ],
      "source": [
        "# Data Collection with robust error handling\n",
        "START_DATE = (pd.Timestamp.today() - pd.DateOffset(years=10)).strftime(\"%Y-%m-%d\")\n",
        "END_DATE = pd.Timestamp.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "TICKERS = {\n",
        "    \"GC=F\": \"Gold\",\n",
        "    \"^TNX\": \"TNX_10Y\",\n",
        "    \"DX-Y.NYB\": \"DXY\",\n",
        "    \"^GSPC\": \"SP500\",\n",
        "    \"^VIX\": \"VIX\",\n",
        "    \"CL=F\": \"CrudeOil\",\n",
        "    \"SI=F\": \"Silver\",\n",
        "}\n",
        "\n",
        "fred_series = \"CPIAUCSL\"\n",
        "\n",
        "try:\n",
        "    # Download market series from yfinance\n",
        "    frames = []\n",
        "    for tkr, alias in TICKERS.items():\n",
        "        df_t = download_yfinance(tkr, START_DATE, END_DATE)\n",
        "        df_t = df_t.rename(columns={tkr: alias})\n",
        "        frames.append(df_t)\n",
        "\n",
        "    # Merge market data on date index\n",
        "    market_df = pd.concat(frames, axis=1)\n",
        "\n",
        "    # CPI is monthly; we'll forward-fill to daily\n",
        "    cpi_df = download_fred(fred_series, START_DATE, END_DATE)\n",
        "    cpi_daily = cpi_df.resample(\"B\").ffill()\n",
        "\n",
        "    # Flatten MultiIndex columns (if any)\n",
        "    if isinstance(market_df.columns, pd.MultiIndex):\n",
        "        market_df.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in market_df.columns]\n",
        "\n",
        "\n",
        "    # Merge all\n",
        "    data = market_df.join(cpi_daily, how=\"outer\")\n",
        "    data = data.sort_index()\n",
        "\n",
        "    # Handle missing values via forward fill then backfill\n",
        "    data = data.ffill().bfill()\n",
        "\n",
        "    # Basic sanity checks\n",
        "    assert not data.isna().sum().sum(), \"Missing values remain after fill.\"\n",
        "    assert data.shape[0] > 0 and data.shape[1] >= 7, \"Insufficient data collected.\"\n",
        "\n",
        "    # Save raw data\n",
        "    data.to_csv(RAW_CSV_PATH, index=True)\n",
        "    log(f\"Raw data saved to {RAW_CSV_PATH}\")\n",
        "\n",
        "    display(data.head())\n",
        "    display(data.tail())\n",
        "    print(\"Data shape:\", data.shape)\n",
        "except Exception as e:\n",
        "    log(f\"Data collection failed: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Preprocessing and Feature Engineering\n",
        "\n",
        "We'll create technical indicators for gold, compute returns and volatility, add RSI, and generate lagged features for all numeric columns. Finally, we'll scale features to [0, 1] using `MinMaxScaler` and save the preprocessed dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Preprocessing failed: 'Gold'\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'Gold'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mKeyError\u001b[39m: 'Gold'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m df = data.copy()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Technical indicators specific to gold\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mGold_MA_7\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGold\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.rolling(window=\u001b[32m7\u001b[39m, min_periods=\u001b[32m7\u001b[39m).mean()\n\u001b[32m      7\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mGold_MA_30\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mGold\u001b[39m\u001b[33m\"\u001b[39m].rolling(window=\u001b[32m30\u001b[39m, min_periods=\u001b[32m30\u001b[39m).mean()\n\u001b[32m      8\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mGold_MA_90\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mGold\u001b[39m\u001b[33m\"\u001b[39m].rolling(window=\u001b[32m90\u001b[39m, min_periods=\u001b[32m90\u001b[39m).mean()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
            "\u001b[31mKeyError\u001b[39m: 'Gold'"
          ]
        }
      ],
      "source": [
        "# Feature Engineering and Scaling\n",
        "try:\n",
        "    df = data.copy()\n",
        "\n",
        "    # Flatten multi-level columns and keep only the renamed tickers\n",
        "    if isinstance(market_df.columns, pd.MultiIndex):\n",
        "        market_df.columns = [col[1] if col[1] else col[0] for col in market_df.columns]\n",
        "\n",
        "    # Technical indicators specific to gold\n",
        "    df[\"Gold_MA_7\"] = df[\"Gold\"].rolling(window=7, min_periods=7).mean()\n",
        "    df[\"Gold_MA_30\"] = df[\"Gold\"].rolling(window=30, min_periods=30).mean()\n",
        "    df[\"Gold_MA_90\"] = df[\"Gold\"].rolling(window=90, min_periods=90).mean()\n",
        "\n",
        "    df[\"Gold_Returns\"] = df[\"Gold\"].pct_change()\n",
        "    df[\"Gold_Volatility_30\"] = df[\"Gold_Returns\"].rolling(window=30, min_periods=30).std()\n",
        "    df[\"Gold_RSI_14\"] = compute_rsi(df[\"Gold\"], period=14)\n",
        "\n",
        "    # Replace any remaining nan from initial rolling windows\n",
        "    df = df.ffill().bfill()\n",
        "\n",
        "    # Create lagged features for all numeric columns\n",
        "    numeric_cols = df.columns.tolist()\n",
        "    lags = [1, 3, 5, 7, 10]\n",
        "    df_lagged = add_lag_features(df, numeric_cols, lags)\n",
        "\n",
        "    # Drop initial rows with nan due to lags\n",
        "    df_lagged = df_lagged.dropna()\n",
        "\n",
        "    # Target is next-day gold price (predict Gold at t)\n",
        "    target_col = \"Gold\"\n",
        "\n",
        "    # Scale all features (including target) to [0, 1]\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_values = scaler.fit_transform(df_lagged.values)\n",
        "    df_scaled = pd.DataFrame(scaled_values, index=df_lagged.index, columns=df_lagged.columns)\n",
        "\n",
        "    # Persist preprocessed data\n",
        "    df_scaled.to_csv(PREP_CSV_PATH)\n",
        "    log(f\"Preprocessed data saved to {PREP_CSV_PATH}\")\n",
        "\n",
        "    display(df.head())\n",
        "    display(df_lagged.head())\n",
        "    display(df_scaled.head())\n",
        "    print(\"Original with indicators shape:\", df.shape)\n",
        "    print(\"Lagged shape:\", df_lagged.shape)\n",
        "    print(\"Scaled shape:\", df_scaled.shape)\n",
        "except Exception as e:\n",
        "    log(f\"Preprocessing failed: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Chronological Split and Sequence Preparation\n",
        "\n",
        "We split the dataset into Train (70%), Validation (15%), and Test (15%) in chronological order. Then we create sequences of length 60 days for the LSTM input and reshape data to `(samples, timesteps, features)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chronological split and sequences\n",
        "SEQ_LEN = 60\n",
        "\n",
        "try:\n",
        "    df_scaled = pd.read_csv(PREP_CSV_PATH, index_col=0, parse_dates=True)\n",
        "    target_col = \"Gold\"\n",
        "\n",
        "    # Determine split indices\n",
        "    n = len(df_scaled)\n",
        "    train_end = int(n * 0.70)\n",
        "    val_end = int(n * 0.85)\n",
        "\n",
        "    # Print date ranges\n",
        "    print(\"Total samples:\", n)\n",
        "    print(\"Train dates:\", df_scaled.index[0].date(), \"->\", df_scaled.index[train_end - 1].date())\n",
        "    print(\"Val dates:\", df_scaled.index[train_end].date(), \"->\", df_scaled.index[val_end - 1].date())\n",
        "    print(\"Test dates:\", df_scaled.index[val_end].date(), \"->\", df_scaled.index[-1].date())\n",
        "\n",
        "    # Split data\n",
        "    train_df = df_scaled.iloc[:train_end]\n",
        "    val_df = df_scaled.iloc[train_end:val_end]\n",
        "    test_df = df_scaled.iloc[val_end:]\n",
        "\n",
        "    # Features and targets (predict scaled Gold)\n",
        "    feature_cols = df_scaled.columns.tolist()\n",
        "    X_train_all = train_df[feature_cols].values\n",
        "    y_train_all = train_df[[target_col]].values.squeeze()\n",
        "\n",
        "    X_val_all = val_df[feature_cols].values\n",
        "    y_val_all = val_df[[target_col]].values.squeeze()\n",
        "\n",
        "    X_test_all = test_df[feature_cols].values\n",
        "    y_test_all = test_df[[target_col]].values.squeeze()\n",
        "\n",
        "    # Create sequences\n",
        "    X_train, y_train = create_sequences(X_train_all, y_train_all, SEQ_LEN)\n",
        "    X_val, y_val = create_sequences(X_val_all, y_val_all, SEQ_LEN)\n",
        "    X_test, y_test = create_sequences(X_test_all, y_test_all, SEQ_LEN)\n",
        "\n",
        "    print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
        "    print(\"X_val:\", X_val.shape, \"y_val:\", y_val.shape)\n",
        "    print(\"X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n",
        "except Exception as e:\n",
        "    log(f\"Split/sequencing failed: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Build and Train LSTM Model\n",
        "\n",
        "We use a two-layer LSTM:\n",
        "- LSTM(50, return_sequences=True) + Dropout(0.2)\n",
        "- LSTM(50, return_sequences=False) + Dropout(0.2)\n",
        "- Dense(25, relu)\n",
        "- Output Dense(1)\n",
        "\n",
        "We compile with Adam and MSE loss, and train up to 100 epochs with EarlyStopping and ModelCheckpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build and train the LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "try:\n",
        "    model = Sequential([\n",
        "        LSTM(50, return_sequences=True, input_shape=(SEQ_LEN, X_train.shape[-1])),\n",
        "        Dropout(0.2),\n",
        "        LSTM(50, return_sequences=False),\n",
        "        Dropout(0.2),\n",
        "        Dense(25, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    model.summary()\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "        ModelCheckpoint(MODEL_PATH, monitor='val_loss', save_best_only=True, verbose=1)\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Ensure best model saved\n",
        "    model.save(MODEL_PATH)\n",
        "    log(f\"Model saved to {MODEL_PATH}\")\n",
        "except Exception as e:\n",
        "    log(f\"Model training failed: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Evaluation and Inverse Transform\n",
        "\n",
        "We evaluate on train, validation, and test sets. We inverse-transform the predictions and targets back to the original scale for metric calculation: RMSE, MAE, MAPE, and RÂ².\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model and inverse transform\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "try:\n",
        "    # Recreate scalers for inverse transform (fit on full lagged data used earlier)\n",
        "    # We will reconstruct an inverse scaler for the target by fitting on the original lagged df\n",
        "    original_df_lagged = add_lag_features(data, data.columns.tolist(), [1,3,5,7,10]).dropna()\n",
        "    target_scaler = MinMaxScaler()\n",
        "    target_scaler.fit(original_df_lagged[[\"Gold\"]])\n",
        "\n",
        "    # Predict\n",
        "    best_model = load_model(MODEL_PATH)\n",
        "    y_train_pred = best_model.predict(X_train).squeeze()\n",
        "    y_val_pred = best_model.predict(X_val).squeeze()\n",
        "    y_test_pred = best_model.predict(X_test).squeeze()\n",
        "\n",
        "    # Inverse transform predictions and truths\n",
        "    y_train_true_inv = target_scaler.inverse_transform(y_train.reshape(-1, 1)).squeeze()\n",
        "    y_val_true_inv = target_scaler.inverse_transform(y_val.reshape(-1, 1)).squeeze()\n",
        "    y_test_true_inv = target_scaler.inverse_transform(y_test.reshape(-1, 1)).squeeze()\n",
        "\n",
        "    y_train_pred_inv = target_scaler.inverse_transform(y_train_pred.reshape(-1, 1)).squeeze()\n",
        "    y_val_pred_inv = target_scaler.inverse_transform(y_val_pred.reshape(-1, 1)).squeeze()\n",
        "    y_test_pred_inv = target_scaler.inverse_transform(y_test_pred.reshape(-1, 1)).squeeze()\n",
        "\n",
        "    # Metrics\n",
        "    results = {\n",
        "        \"Train\": {\n",
        "            \"RMSE\": math.sqrt(mean_squared_error(y_train_true_inv, y_train_pred_inv)),\n",
        "            \"MAE\": mean_absolute_error(y_train_true_inv, y_train_pred_inv),\n",
        "            \"MAPE\": mape(y_train_true_inv, y_train_pred_inv),\n",
        "            \"R2\": r2_score(y_train_true_inv, y_train_pred_inv),\n",
        "        },\n",
        "        \"Validation\": {\n",
        "            \"RMSE\": math.sqrt(mean_squared_error(y_val_true_inv, y_val_pred_inv)),\n",
        "            \"MAE\": mean_absolute_error(y_val_true_inv, y_val_pred_inv),\n",
        "            \"MAPE\": mape(y_val_true_inv, y_val_pred_inv),\n",
        "            \"R2\": r2_score(y_val_true_inv, y_val_pred_inv),\n",
        "        },\n",
        "        \"Test\": {\n",
        "            \"RMSE\": math.sqrt(mean_squared_error(y_test_true_inv, y_test_pred_inv)),\n",
        "            \"MAE\": mean_absolute_error(y_test_true_inv, y_test_pred_inv),\n",
        "            \"MAPE\": mape(y_test_true_inv, y_test_pred_inv),\n",
        "            \"R2\": r2_score(y_test_true_inv, y_test_pred_inv),\n",
        "        },\n",
        "    }\n",
        "\n",
        "    print_metrics_table(results)\n",
        "except Exception as e:\n",
        "    log(f\"Evaluation failed: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Visualization\n",
        "\n",
        "We produce:\n",
        "1. Training history (loss over epochs)\n",
        "2. Actual vs Predicted (train, val, test)\n",
        "3. Prediction error distribution\n",
        "4. Feature correlation heatmap\n",
        "5. Last 100 days actual vs predicted with confidence bounds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizations\n",
        "try:\n",
        "    # 1) Training history\n",
        "    hist_df = pd.DataFrame(history.history)\n",
        "    fig, ax = plt.subplots()\n",
        "    hist_df[[\"loss\", \"val_loss\"]].plot(ax=ax)\n",
        "    ax.set_title(\"Training History: Loss over Epochs\")\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"MSE Loss\")\n",
        "    ax.legend([\"Train Loss\", \"Val Loss\"])\n",
        "    plt.show()\n",
        "\n",
        "    # 2) Actual vs Predicted\n",
        "    # Reconstruct aligned dates for sequences\n",
        "    train_dates = train_df.index[SEQ_LEN:]\n",
        "    val_dates = val_df.index[SEQ_LEN:]\n",
        "    test_dates = test_df.index[SEQ_LEN:]\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=train_dates, y=y_train_true_inv, mode='lines', name='Train Actual'))\n",
        "    fig.add_trace(go.Scatter(x=train_dates, y=y_train_pred_inv, mode='lines', name='Train Pred'))\n",
        "\n",
        "    fig.add_trace(go.Scatter(x=val_dates, y=y_val_true_inv, mode='lines', name='Val Actual'))\n",
        "    fig.add_trace(go.Scatter(x=val_dates, y=y_val_pred_inv, mode='lines', name='Val Pred'))\n",
        "\n",
        "    fig.add_trace(go.Scatter(x=test_dates, y=y_test_true_inv, mode='lines', name='Test Actual'))\n",
        "    fig.add_trace(go.Scatter(x=test_dates, y=y_test_pred_inv, mode='lines', name='Test Pred'))\n",
        "\n",
        "    fig.update_layout(title='Actual vs Predicted Gold Prices', xaxis_title='Date', yaxis_title='Price', template='plotly_white')\n",
        "    fig.show()\n",
        "\n",
        "    # 3) Prediction error distribution (test)\n",
        "    errors = y_test_true_inv - y_test_pred_inv\n",
        "    plt.figure(figsize=(10,6))\n",
        "    sns.histplot(errors, bins=50, kde=True)\n",
        "    plt.title(\"Prediction Error Distribution (Test)\")\n",
        "    plt.xlabel(\"Error (Actual - Pred)\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n",
        "\n",
        "    # 4) Feature correlation heatmap (on scaled features)\n",
        "    corr = df_scaled.corr()\n",
        "    plt.figure(figsize=(14,10))\n",
        "    sns.heatmap(corr, cmap='coolwarm', center=0, cbar_kws={'shrink': .6})\n",
        "    plt.title(\"Feature Correlation Heatmap (Scaled)\")\n",
        "    plt.show()\n",
        "\n",
        "    # 5) Last 100 days with confidence bounds (using residual std)\n",
        "    last_n = 100\n",
        "    last_dates = test_dates[-last_n:]\n",
        "    last_true = y_test_true_inv[-last_n:]\n",
        "    last_pred = y_test_pred_inv[-last_n:]\n",
        "    resid_std = np.std(errors)\n",
        "    upper = last_pred + 1.96 * resid_std\n",
        "    lower = last_pred - 1.96 * resid_std\n",
        "\n",
        "    fig2 = go.Figure()\n",
        "    fig2.add_trace(go.Scatter(x=last_dates, y=last_true, mode='lines', name='Actual'))\n",
        "    fig2.add_trace(go.Scatter(x=last_dates, y=last_pred, mode='lines', name='Predicted'))\n",
        "    fig2.add_trace(go.Scatter(x=last_dates, y=upper, mode='lines', name='Upper 95%', line=dict(dash='dash')))\n",
        "    fig2.add_trace(go.Scatter(x=last_dates, y=lower, mode='lines', name='Lower 95%', line=dict(dash='dash')))\n",
        "    fig2.update_layout(title='Last 100 Days: Prediction with Confidence Bounds', xaxis_title='Date', yaxis_title='Price', template='plotly_white')\n",
        "    fig2.show()\n",
        "except Exception as e:\n",
        "    log(f\"Visualization failed: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Future Predictions (Next 7 Days)\n",
        "\n",
        "We use the last 60 days of scaled features to iteratively predict the next 7 days. Predictions are inverse-transformed to the original price scale and displayed with dates and a plot for context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Future predictions (7 days)\n",
        "try:\n",
        "    # We'll build from the most recent scaled frame\n",
        "    full_scaled = df_scaled.copy()\n",
        "\n",
        "    # Prepare scalers for inverse transform of target\n",
        "    original_df_lagged = add_lag_features(data, data.columns.tolist(), [1,3,5,7,10]).dropna()\n",
        "    target_scaler = MinMaxScaler()\n",
        "    target_scaler.fit(original_df_lagged[[\"Gold\"]])\n",
        "\n",
        "    # Use last SEQ_LEN rows from the full scaled dataset\n",
        "    recent_window = full_scaled.values[-SEQ_LEN:]\n",
        "\n",
        "    future_scaled_preds = []\n",
        "    future_dates = []\n",
        "\n",
        "    # We only predict the target (Gold) one step ahead each iteration, without updating other features\n",
        "    # For a production system, consider a full-feature recursive strategy.\n",
        "    current_window = recent_window.copy()\n",
        "    for i in range(7):\n",
        "        x_input = current_window.reshape(1, SEQ_LEN, full_scaled.shape[1])\n",
        "        next_scaled = best_model.predict(x_input).squeeze()\n",
        "        future_scaled_preds.append(next_scaled)\n",
        "        next_date = (df_scaled.index[-1] + pd.Timedelta(days=1+i))\n",
        "        # Move window forward by appending a vector where only target is updated\n",
        "        next_row = current_window[-1].copy()\n",
        "        # Target column index\n",
        "        target_idx = list(full_scaled.columns).index(\"Gold\")\n",
        "        next_row[target_idx] = next_scaled\n",
        "        current_window = np.vstack([current_window[1:], next_row])\n",
        "        future_dates.append(next_date)\n",
        "\n",
        "    # Inverse transform predictions\n",
        "    future_preds_inv = target_scaler.inverse_transform(np.array(future_scaled_preds).reshape(-1, 1)).squeeze()\n",
        "\n",
        "    # Display table\n",
        "    future_df = pd.DataFrame({\"Date\": pd.to_datetime(future_dates), \"Predicted_Gold_Price\": future_preds_inv})\n",
        "    display(future_df)\n",
        "\n",
        "    # Plot with historical context\n",
        "    history_days = 200\n",
        "    context_dates = test_dates[-history_days:]\n",
        "    context_true = y_test_true_inv[-history_days:]\n",
        "\n",
        "    fig3 = go.Figure()\n",
        "    fig3.add_trace(go.Scatter(x=context_dates, y=context_true, mode='lines', name='Recent Actual'))\n",
        "    fig3.add_trace(go.Scatter(x=future_df[\"Date\"], y=future_df[\"Predicted_Gold_Price\"], mode='lines+markers', name='Future Pred (7d)'))\n",
        "    fig3.update_layout(title='Future 7-Day Forecast with Historical Context', xaxis_title='Date', yaxis_title='Price', template='plotly_white')\n",
        "    fig3.show()\n",
        "except Exception as e:\n",
        "    log(f\"Future prediction failed: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "In this notebook, we built a robust, end-to-end LSTM pipeline for gold price prediction, including data collection, preprocessing, modeling, evaluation, visualization, and short-term forecasting. Consider improving the model by:\n",
        "\n",
        "- Adding exogenous feature engineering (macro surprises, term spreads, commodity spreads)\n",
        "- Testing alternative sequence lengths, architectures (GRU, TCN), and hyperparameters\n",
        "- Using walk-forward validation for more realistic evaluation\n",
        "- Calibrating prediction intervals with quantile models or bootstrapping\n",
        "- Incorporating regime detection (volatility states) for conditional modeling\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
