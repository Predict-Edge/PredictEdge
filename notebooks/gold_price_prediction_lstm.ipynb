{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Price Prediction with LSTM (End-to-End)\n",
    "\n",
    "This notebook builds a complete, production-ready pipeline to predict daily gold prices using Long Short-Term Memory (LSTM) neural networks. It includes:\n",
    "\n",
    "- **Data Collection**: Using `scripts/download_data.py` to fetch and save raw data\n",
    "- **Data Preprocessing**: Feature engineering, scaling, and saving preprocessed data\n",
    "- **Model Training**: Using `scripts/train_lstm.py` for training with proper model persistence\n",
    "- **Model Evaluation**: Computing metrics and saving results to CSV\n",
    "- **Visualization**: Creating publication-ready plots saved to `data/processed/plots/`\n",
    "- **Future Predictions**: Generating 7-day forecasts using `scripts/run_prediction.py`\n",
    "\n",
    "This notebook follows best practices:\n",
    "- Uses relative imports from scripts/\n",
    "- Saves all intermediate results to appropriate folders\n",
    "- Uses os.path.join() or pathlib.Path for all file paths\n",
    "- Includes error handling and progress messages\n",
    "- Sets random seed (42) for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Set random seeds for reproducibility, import required libraries, configure paths, and add utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Print progress helper\n",
    "def log(msg: str) -> None:\n",
    "    print(f\"[INFO] {msg}\")\n",
    "\n",
    "# Add scripts directory to path for imports\n",
    "NOTEBOOK_DIR = Path.cwd() if \"__file__\" not in globals() else Path(__file__).parent\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "SCRIPTS_DIR = PROJECT_ROOT / \"scripts\"\n",
    "sys.path.insert(0, str(SCRIPTS_DIR))\n",
    "\n",
    "# Reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    tf.keras.utils.set_random_seed(42)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "except Exception as e:\n",
    "    log(\"TensorFlow determinism not fully enabled; continuing.\")\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Import from scripts\n",
    "from download_data import main as download_data_main\n",
    "from preprocess_data import (\n",
    "    preprocess_gold_data,\n",
    "    split_and_create_sequences,\n",
    "    save_splits\n",
    ")\n",
    "from train_lstm import (\n",
    "    set_random_seed,\n",
    "    train_lstm_model,\n",
    "    save_model_with_timestamp\n",
    ")\n",
    "from run_prediction import (\n",
    "    predict_future,\n",
    "    make_predictions,\n",
    "    load_model_and_scaler\n",
    ")\n",
    "from evaluate_model import (\n",
    "    evaluate_model,\n",
    "    save_evaluation_results,\n",
    "    print_metrics_table\n",
    ")\n",
    "from visualize_results import (\n",
    "    plot_training_history,\n",
    "    plot_actual_vs_predicted,\n",
    "    plot_error_distribution,\n",
    "    plot_feature_correlation,\n",
    "    plot_last_n_days_with_confidence,\n",
    "    plot_future_predictions\n",
    ")\n",
    "\n",
    "# Define paths using pathlib\n",
    "BASE_DIR = PROJECT_ROOT\n",
    "DATA_RAW_DIR = BASE_DIR / \"data\" / \"raw\"\n",
    "DATA_PROCESSED_DIR = BASE_DIR / \"data\" / \"processed\"\n",
    "DATA_PROCESSED_TRAIN_DIR = DATA_PROCESSED_DIR / \"train\"\n",
    "DATA_PROCESSED_VAL_DIR = DATA_PROCESSED_DIR / \"val\"\n",
    "DATA_PROCESSED_TEST_DIR = DATA_PROCESSED_DIR / \"test\"\n",
    "DATA_PROCESSED_PLOTS_DIR = DATA_PROCESSED_DIR / \"plots\"\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_path in [DATA_RAW_DIR, DATA_PROCESSED_DIR, DATA_PROCESSED_TRAIN_DIR,\n",
    "                 DATA_PROCESSED_VAL_DIR, DATA_PROCESSED_TEST_DIR,\n",
    "                 DATA_PROCESSED_PLOTS_DIR, MODELS_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# File paths\n",
    "RAW_DATA_PATH = DATA_RAW_DIR / \"gold_raw_data.csv\"\n",
    "PREPROCESSED_DATA_PATH = DATA_PROCESSED_DIR / \"gold_preprocessed_data.csv\"\n",
    "EVALUATION_RESULTS_PATH = MODELS_DIR / \"evaluation_results.csv\"\n",
    "FUTURE_PREDICTIONS_PATH = DATA_PROCESSED_DIR / \"future_predictions.csv\"\n",
    "\n",
    "log(\"Environment initialized.\")\n",
    "log(f\"Project root: {BASE_DIR}\")\n",
    "log(f\"Data raw directory: {DATA_RAW_DIR}\")\n",
    "log(f\"Data processed directory: {DATA_PROCESSED_DIR}\")\n",
    "log(f\"Models directory: {MODELS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Data Collection\n",
    "\n",
    "Download historical gold price data and economic indicators using `scripts/download_data.py`. The script will:\n",
    "- Download 10 years of data for Gold Futures (`GC=F`) and economic indicators\n",
    "- Download CPI from FRED (`CPIAUCSL`)\n",
    "- Merge all datasets and save to `data/raw/gold_raw_data.csv`\n",
    "\n",
    "If the data already exists, we'll load it from the file instead of downloading again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection: Use download_data.py script or load existing data\n",
    "try:\n",
    "    if RAW_DATA_PATH.exists():\n",
    "        log(f\"Raw data already exists at {RAW_DATA_PATH}. Loading...\")\n",
    "        raw_data = pd.read_csv(RAW_DATA_PATH, index_col=0, parse_dates=True)\n",
    "        log(f\"Loaded raw data: {raw_data.shape}\")\n",
    "    else:\n",
    "        log(\"Raw data not found. Downloading...\")\n",
    "        # Run download script\n",
    "        import subprocess\n",
    "        result = subprocess.run(\n",
    "            [\"python\", str(SCRIPTS_DIR / \"download_data.py\"),\n",
    "             \"--output-root\", str(DATA_RAW_DIR)],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            cwd=str(PROJECT_ROOT)\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            log(\"Download completed successfully.\")\n",
    "            raw_data = pd.read_csv(RAW_DATA_PATH, index_col=0, parse_dates=True)\n",
    "        else:\n",
    "            log(f\"Download script error: {result.stderr}\")\n",
    "            raise RuntimeError(\"Data download failed\")\n",
    "    \n",
    "    log(f\"Raw data shape: {raw_data.shape}\")\n",
    "    log(f\"Raw data columns: {raw_data.columns.tolist()}\")\n",
    "    log(f\"Date range: {raw_data.index.min()} to {raw_data.index.max()}\")\n",
    "    \n",
    "    # Display sample\n",
    "    display(raw_data.head())\n",
    "    display(raw_data.tail())\n",
    "    \n",
    "except Exception as e:\n",
    "    log(f\"Data collection failed: {e}\")\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Data Preprocessing and Feature Engineering\n",
    "\n",
    "Preprocess the raw data using `scripts/preprocess_data.py` functions:\n",
    "- Create technical indicators (MA, RSI, returns, volatility)\n",
    "- Add lag features for all numeric columns\n",
    "- Scale features to [0, 1] using MinMaxScaler\n",
    "- Save preprocessed data to `data/processed/gold_preprocessed_data.csv`\n",
    "- Split data chronologically and create LSTM sequences\n",
    "- Save train/val/test splits to their respective folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing: Feature engineering and scaling\n",
    "try:\n",
    "    log(\"Starting preprocessing...\")\n",
    "    \n",
    "    # Preprocess data\n",
    "    preprocessed_data, feature_scaler = preprocess_gold_data(\n",
    "        raw_data,\n",
    "        target_col=\"Gold\",\n",
    "        lags=[1, 3, 5, 7, 10]\n",
    "    )\n",
    "    \n",
    "    log(f\"Preprocessed data shape: {preprocessed_data.shape}\")\n",
    "    \n",
    "    # Save preprocessed data\n",
    "    preprocessed_data.to_csv(PREPROCESSED_DATA_PATH)\n",
    "    log(f\"Preprocessed data saved to {PREPROCESSED_DATA_PATH}\")\n",
    "    \n",
    "    # Display sample\n",
    "    display(preprocessed_data.head())\n",
    "    display(preprocessed_data.tail())\n",
    "    \n",
    "    # Split and create sequences\n",
    "    SEQ_LEN = 60\n",
    "    log(f\"Creating sequences with length {SEQ_LEN}...\")\n",
    "    \n",
    "    splits = split_and_create_sequences(\n",
    "        preprocessed_data,\n",
    "        target_col=\"Gold\",\n",
    "        seq_len=SEQ_LEN,\n",
    "        train_ratio=0.70,\n",
    "        val_ratio=0.15\n",
    "    )\n",
    "    \n",
    "    # Extract sequences\n",
    "    X_train, y_train, train_dates = splits[\"train\"]\n",
    "    X_val, y_val, val_dates = splits[\"val\"]\n",
    "    X_test, y_test, test_dates = splits[\"test\"]\n",
    "    \n",
    "    # Save splits\n",
    "    save_splits(preprocessed_data, splits, DATA_PROCESSED_DIR, target_col=\"Gold\")\n",
    "    \n",
    "    log(\"Preprocessing complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    log(f\"Preprocessing failed: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: Model Training\n",
    "\n",
    "Train the LSTM model using `scripts/train_lstm.py`:\n",
    "- Build two-layer LSTM with dropout\n",
    "- Train with early stopping\n",
    "- Save model and scaler with timestamp to `models/` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training: Build and train LSTM\n",
    "try:\n",
    "    log(\"Starting model training...\")\n",
    "    \n",
    "    # Train model\n",
    "    model, history = train_lstm_model(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        patience=10,\n",
    "        lstm_units=50,\n",
    "        dropout_rate=0.2,\n",
    "        dense_units=25,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Save model and scaler with timestamp\n",
    "    model_path, scaler_path = save_model_with_timestamp(\n",
    "        model,\n",
    "        feature_scaler,\n",
    "        MODELS_DIR,\n",
    "        prefix=\"gold_lstm\"\n",
    "    )\n",
    "    \n",
    "    log(\"Model training complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    log(f\"Model training failed: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: Model Evaluation\n",
    "\n",
    "Evaluate the model on train, validation, and test sets:\n",
    "- Make predictions on all splits\n",
    "- Inverse transform predictions back to original scale\n",
    "- Compute metrics (RMSE, MAE, MAPE, R²)\n",
    "- Save evaluation results to `models/evaluation_results.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation: Compute metrics and save results\n",
    "try:\n",
    "    log(\"Starting model evaluation...\")\n",
    "    \n",
    "    # Find target column index for inverse transform\n",
    "    target_col_idx = list(preprocessed_data.columns).index(\"Gold\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred_scaled = model.predict(X_train, verbose=0).squeeze()\n",
    "    y_val_pred_scaled = model.predict(X_val, verbose=0).squeeze()\n",
    "    y_test_pred_scaled = model.predict(X_test, verbose=0).squeeze()\n",
    "    \n",
    "    # Inverse transform using the feature scaler\n",
    "    # Create dummy arrays with target at correct position\n",
    "    n_train = len(y_train)\n",
    "    n_val = len(y_val)\n",
    "    n_test = len(y_test)\n",
    "    n_features = preprocessed_data.shape[1]\n",
    "    \n",
    "    # Inverse transform true values\n",
    "    train_true_full = np.zeros((n_train, n_features))\n",
    "    train_true_full[:, target_col_idx] = y_train\n",
    "    y_train_true_inv = feature_scaler.inverse_transform(train_true_full)[:, target_col_idx]\n",
    "    \n",
    "    val_true_full = np.zeros((n_val, n_features))\n",
    "    val_true_full[:, target_col_idx] = y_val\n",
    "    y_val_true_inv = feature_scaler.inverse_transform(val_true_full)[:, target_col_idx]\n",
    "    \n",
    "    test_true_full = np.zeros((n_test, n_features))\n",
    "    test_true_full[:, target_col_idx] = y_test\n",
    "    y_test_true_inv = feature_scaler.inverse_transform(test_true_full)[:, target_col_idx]\n",
    "    \n",
    "    # Inverse transform predictions\n",
    "    train_pred_full = np.zeros((n_train, n_features))\n",
    "    train_pred_full[:, target_col_idx] = y_train_pred_scaled\n",
    "    y_train_pred_inv = feature_scaler.inverse_transform(train_pred_full)[:, target_col_idx]\n",
    "    \n",
    "    val_pred_full = np.zeros((n_val, n_features))\n",
    "    val_pred_full[:, target_col_idx] = y_val_pred_scaled\n",
    "    y_val_pred_inv = feature_scaler.inverse_transform(val_pred_full)[:, target_col_idx]\n",
    "    \n",
    "    test_pred_full = np.zeros((n_test, n_features))\n",
    "    test_pred_full[:, target_col_idx] = y_test_pred_scaled\n",
    "    y_test_pred_inv = feature_scaler.inverse_transform(test_pred_full)[:, target_col_idx]\n",
    "    \n",
    "    # Evaluate model\n",
    "    results_df = evaluate_model(\n",
    "        y_train_true_inv, y_train_pred_inv,\n",
    "        y_val_true_inv, y_val_pred_inv,\n",
    "        y_test_true_inv, y_test_pred_inv\n",
    "    )\n",
    "    \n",
    "    # Print metrics\n",
    "    print_metrics_table(results_df)\n",
    "    \n",
    "    # Save evaluation results\n",
    "    save_evaluation_results(results_df, EVALUATION_RESULTS_PATH, append=False)\n",
    "    \n",
    "    log(\"Evaluation complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    log(f\"Evaluation failed: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5: Visualization\n",
    "\n",
    "Create publication-ready visualizations and save them to `data/processed/plots/`:\n",
    "1. Training/validation loss curves\n",
    "2. Actual vs Predicted prices for train/val/test\n",
    "3. Prediction error distribution\n",
    "4. Feature correlation heatmap\n",
    "5. Last 100 days with confidence bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization: Create and save all plots\n",
    "try:\n",
    "    log(\"Creating visualizations...\")\n",
    "    \n",
    "    # 1. Training history\n",
    "    plot_training_history(\n",
    "        history,\n",
    "        output_path=DATA_PROCESSED_PLOTS_DIR / \"training_history.png\"\n",
    "    )\n",
    "    \n",
    "    # 2. Actual vs Predicted\n",
    "    plot_actual_vs_predicted(\n",
    "        train_dates, y_train_true_inv, y_train_pred_inv,\n",
    "        val_dates, y_val_true_inv, y_val_pred_inv,\n",
    "        test_dates, y_test_true_inv, y_test_pred_inv,\n",
    "        output_path=DATA_PROCESSED_PLOTS_DIR / \"actual_vs_predicted.png\"\n",
    "    )\n",
    "    \n",
    "    # 3. Error distribution\n",
    "    test_errors = y_test_true_inv - y_test_pred_inv\n",
    "    plot_error_distribution(\n",
    "        test_errors,\n",
    "        split_name=\"Test\",\n",
    "        output_path=DATA_PROCESSED_PLOTS_DIR / \"error_distribution.png\"\n",
    "    )\n",
    "    \n",
    "    # 4. Feature correlation heatmap\n",
    "    plot_feature_correlation(\n",
    "        preprocessed_data,\n",
    "        output_path=DATA_PROCESSED_PLOTS_DIR / \"feature_correlation.png\"\n",
    "    )\n",
    "    \n",
    "    # 5. Last 100 days with confidence bands\n",
    "    plot_last_n_days_with_confidence(\n",
    "        test_dates, y_test_true_inv, y_test_pred_inv,\n",
    "        n_days=100,\n",
    "        output_path=DATA_PROCESSED_PLOTS_DIR / \"last_100_days_confidence.png\"\n",
    "    )\n",
    "    \n",
    "    log(\"Visualizations complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    log(f\"Visualization failed: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 6: Future Predictions\n",
    "\n",
    "Generate 7-day future predictions using `scripts/run_prediction.py`:\n",
    "- Use the last sequence to predict next 7 days\n",
    "- Save predictions to `data/processed/future_predictions.csv`\n",
    "- Visualize future predictions with historical context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Predictions: Generate 7-day forecast\n",
    "try:\n",
    "    log(\"Generating future predictions...\")\n",
    "    \n",
    "    # Get last sequence for prediction\n",
    "    last_sequence = preprocessed_data.values[-SEQ_LEN:]\n",
    "    target_col_idx = list(preprocessed_data.columns).index(\"Gold\")\n",
    "    \n",
    "    # Generate future predictions\n",
    "    future_predictions, future_dates = predict_future(\n",
    "        model,\n",
    "        feature_scaler,\n",
    "        last_sequence,\n",
    "        n_days=7,\n",
    "        target_col_idx=target_col_idx\n",
    "    )\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    future_df = pd.DataFrame({\n",
    "        \"Date\": future_dates,\n",
    "        \"Predicted_Gold_Price\": future_predictions\n",
    "    })\n",
    "    future_df.to_csv(FUTURE_PREDICTIONS_PATH, index=False)\n",
    "    log(f\"Future predictions saved to {FUTURE_PREDICTIONS_PATH}\")\n",
    "    \n",
    "    # Display predictions\n",
    "    display(future_df)\n",
    "    \n",
    "    # Plot with historical context\n",
    "    plot_future_predictions(\n",
    "        test_dates, y_test_true_inv,\n",
    "        future_dates, future_predictions,\n",
    "        n_history=200,\n",
    "        output_path=DATA_PROCESSED_PLOTS_DIR / \"future_predictions.png\"\n",
    "    )\n",
    "    \n",
    "    log(\"Future predictions complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    log(f\"Future prediction failed: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook has successfully:\n",
    "- ✅ Collected raw data and saved to `data/raw/gold_raw_data.csv`\n",
    "- ✅ Preprocessed data with feature engineering and saved to `data/processed/`\n",
    "- ✅ Trained LSTM model and saved with timestamp to `models/`\n",
    "- ✅ Evaluated model and saved results to `models/evaluation_results.csv`\n",
    "- ✅ Created visualizations and saved to `data/processed/plots/`\n",
    "- ✅ Generated 7-day future predictions and saved to `data/processed/future_predictions.csv`\n",
    "\n",
    "**Consider improving the model by:**\n",
    "- Adding exogenous feature engineering (macro surprises, term spreads, commodity spreads)\n",
    "- Testing alternative sequence lengths, architectures (GRU, TCN), and hyperparameters\n",
    "- Using walk-forward validation for more realistic evaluation\n",
    "- Calibrating prediction intervals with quantile models or bootstrapping\n",
    "- Incorporating regime detection (volatility states) for conditional modeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
